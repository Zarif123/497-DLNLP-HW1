{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO1EBavGJCfE"
      },
      "source": [
        "## 1. Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFl39aPlJCfH"
      },
      "source": [
        "### 1.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gkDLnPZoJCfI"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvk7-cAuJCfJ"
      },
      "source": [
        "### 1.2 Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X7k-SHZPJCfJ"
      },
      "outputs": [],
      "source": [
        "# Encoding and decoding\n",
        "def decode(vocab,corpus):\n",
        "    \n",
        "    text = ''\n",
        "    for i in range(len(corpus)):\n",
        "        wID = corpus[i]\n",
        "        text = text + vocab[wID] + ' '\n",
        "    return(text)\n",
        "\n",
        "def encode(words,text):\n",
        "    corpus = []\n",
        "    tokens = text.split(' ')\n",
        "    for t in tokens:\n",
        "        try:\n",
        "            wID = words[t][0]\n",
        "        except:\n",
        "            wID = words['<unk>'][0]\n",
        "        corpus.append(wID)\n",
        "    return(corpus)\n",
        "\n",
        "def read_encode(file_name,vocab,words,corpus,threshold):\n",
        "    \n",
        "    wID = len(vocab)\n",
        "    \n",
        "    if threshold > -1:\n",
        "        with open(file_name,'rt', encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                line = line.replace('\\n','')\n",
        "                tokens = line.split(' ')\n",
        "                for t in tokens:\n",
        "                    try:\n",
        "                        elem = words[t]\n",
        "                    except:\n",
        "                        elem = [wID,0]\n",
        "                        vocab.append(t)\n",
        "                        wID = wID + 1\n",
        "                    elem[1] = elem[1] + 1\n",
        "                    words[t] = elem\n",
        "\n",
        "        temp = words\n",
        "        words = {}\n",
        "        vocab = []\n",
        "        wID = 0\n",
        "        words['<unk>'] = [wID,100]\n",
        "        vocab.append('<unk>')\n",
        "        for t in temp:\n",
        "            if temp[t][1] >= threshold:\n",
        "                vocab.append(t)\n",
        "                wID = wID + 1\n",
        "                words[t] = [wID,temp[t][1]]\n",
        "            \n",
        "                    \n",
        "    with open(file_name,'rt', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            line = line.replace('\\n','')\n",
        "            tokens = line.split(' ')\n",
        "            for t in tokens:\n",
        "                try:\n",
        "                    wID = words[t][0]\n",
        "                except:\n",
        "                    wID = words['<unk>'][0]\n",
        "                corpus.append(wID)\n",
        "                \n",
        "    return [vocab,words,corpus]\n",
        "\n",
        "def plot_data(x, y1, y2, xlabel, ylabel):\n",
        "    plt.plot(x, y1, 'b')\n",
        "    plt.plot(x, y2, 'r')\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qmUIGuxJCfL"
      },
      "source": [
        "### 1.3 Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UCT_T7uXJCfL"
      },
      "outputs": [],
      "source": [
        "# CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s6bWNUfJCfL"
      },
      "source": [
        "## 2. Data-Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKjBABtmJCfM"
      },
      "source": [
        "### 2.1 Creating DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2518Cc6dJCfM"
      },
      "outputs": [],
      "source": [
        "def create_df(bios, targets):\n",
        "    zipped_data = list(zip(bios, targets))\n",
        "    text_df = pd.DataFrame(zipped_data, columns=['Bios', 'Labels'])\n",
        "    return text_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWJH9nTlJCfM"
      },
      "source": [
        "### 2.2 Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SE-x73GkJCfN"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "def process_data(data, words):\n",
        "    fake_i = (data == words[\"[FAKE]\"][0])\n",
        "    real_i = (data == words[\"[REAL]\"][0])\n",
        "    target_indices = (fake_i + real_i).nonzero()\n",
        "    num_entries = len(target_indices)\n",
        "    bio_tensor_list = []\n",
        "    target_list = []\n",
        "    entry = 0\n",
        "    start_i = 0\n",
        "\n",
        "    while entry < num_entries:\n",
        "        target_i = target_indices[entry]\n",
        "\n",
        "        # Size of data and targets\n",
        "        #print(f\"Data size: {data[start_i:target_i].size()}\")\n",
        "        #print(f\"Target size: {data[target_i].size()}\")\n",
        "\n",
        "        # Take in a list of tensors and use pad sequence\n",
        "        bio = data[start_i:target_i]\n",
        "        target = [1, 0] if data[target_i] == words[\"[FAKE]\"][0] else [0, 1]\n",
        "\n",
        "        bio_tensor_list.append(torch.tensor(bio).squeeze())\n",
        "        target_list.append(target)\n",
        "\n",
        "        start_i = target_i + 1\n",
        "        entry += 1\n",
        "\n",
        "    # have largest size be 880\n",
        "    pad_bias = torch.ones(880)\n",
        "    bio_tensor_list.append(pad_bias)\n",
        "\n",
        "    padded_bios = torch.t(pad_sequence(bio_tensor_list)).to(device)\n",
        "    padded_bios = padded_bios[:-1]\n",
        "    targets = torch.tensor(target_list).to(device)\n",
        "\n",
        "    # print(\"First bio\", padded_bios[0].size())\n",
        "    # print(\"Second bio\", padded_bios[1].size())\n",
        "    data_df = create_df(padded_bios, targets)\n",
        "    return data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTIfJaoFJCfN"
      },
      "source": [
        "### 2.2.5 Pre-process n-gram Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JXyfld2fJCfN"
      },
      "outputs": [],
      "source": [
        "def process_ngram(data, words):\n",
        "\n",
        "    # Features\n",
        "    data = torch.tensor(data)\n",
        "    new_len = (math.floor(math.sqrt(len(data))) + 1) ** 2\n",
        "    pad_arr = [words['<unk>'][0] for i in range(new_len - len(data))]\n",
        "    pad_arr = torch.tensor(pad_arr)\n",
        "\n",
        "    padded_data = torch.cat((data, pad_arr), dim=0)\n",
        "    padded_data = torch.reshape(padded_data, (int(math.sqrt(new_len)), int(math.sqrt(new_len))))\n",
        "    padded_data = list(map(torch.tensor, padded_data))\n",
        "    print(len(padded_data))\n",
        "    print(padded_data[0].size())\n",
        "    data_df = pd.DataFrame(list(zip(padded_data)), columns=['Bios'])\n",
        "\n",
        "    return data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GubxWu6gJCfN"
      },
      "source": [
        "### 2.3 Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rjRFR5FuJCfN"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data_df):\n",
        "        self.text_df = data_df\n",
        "        print(\"Head of Data\", self.text_df.head())\n",
        "        self.x = self.text_df['Bios']\n",
        "        #self.y = self.text_df['Labels']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx]#, self.y[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF_-ZAouJCfO"
      },
      "source": [
        "## 3. Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehrvXzuNJCfO"
      },
      "source": [
        "### 3.1 FFNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B0rJBfeRJCfO"
      },
      "outputs": [],
      "source": [
        "# FeedForward Model\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, vocab, words, d_model, d_hidden, dropout):\n",
        "        super().__init__() \n",
        "    \n",
        "        # Class parameters\n",
        "        self.vocab = vocab\n",
        "        self.words = words\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.d_model = d_model\n",
        "        self.d_hidden = d_hidden\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embeds = nn.Embedding(self.vocab_size, self.d_model)\n",
        "\n",
        "        # Linear Layers\n",
        "        self.fc1 = nn.Linear(880 * d_model, d_hidden)\n",
        "        self.fc2 = nn.Linear(d_hidden, 2)\n",
        "\n",
        "        # Nonlinear Layer\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "        # Setting weights\n",
        "        self.init_weights()\n",
        "                \n",
        "    # Initialize weights for foward layer\n",
        "    def init_weights(self):\n",
        "        weight_range = 0.1\n",
        "        self.embeds.weight.data.uniform_(-weight_range, weight_range)\n",
        "        self.fc1.weight.data.uniform_(-weight_range, weight_range)\n",
        "        self.fc1.bias.data.zero_()\n",
        "\n",
        "    # Forward\n",
        "    def forward(self, src):\n",
        "        # Embeddings are fed into the forward layer\n",
        "        embeds = self.embeds(src).view((-1, 880 * self.d_model))\n",
        "        x = self.dropout(self.activation(self.fc1(embeds)))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwzDTQVaJCfO"
      },
      "source": [
        "### 3.2 Language FFNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fX8bUP9RJCfO"
      },
      "outputs": [],
      "source": [
        "# FeedForward Model\n",
        "class LanguageFFNN(nn.Module):\n",
        "    def __init__(self, vocab, words, d_model, d_hidden, dropout, ngram_size):\n",
        "        super().__init__() \n",
        "    \n",
        "        # Class parameters\n",
        "        self.vocab = vocab\n",
        "        self.words = words\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.d_model = d_model\n",
        "        self.d_hidden = d_hidden\n",
        "        self.ngram_size = ngram_size\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Embedding Layers\n",
        "        self.input_embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
        "        # self.output_embedding =  nn.Embedding(self.vocab_size, self.d_model)\n",
        "        \n",
        "        # self.embeds = nn.Embedding(self.vocab_size, self.d_model)\n",
        "\n",
        "        # Linear Layers\n",
        "        self.fc1 = nn.Linear(ngram_size * d_model, self.d_model)\n",
        "        # self.fc2 = nn.Linear(d_hidden, self.vocab_size)\n",
        "        self.output_embedding = nn.Linear(self.d_model, self.vocab_size)\n",
        "\n",
        "        # Nonlinear Layer\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "        # Setting weights\n",
        "        self.init_weights()\n",
        "                \n",
        "    # Initialize weights for foward layer\n",
        "    def init_weights(self):\n",
        "        weight_range = 0.1\n",
        "        # tie embedding layers\n",
        "        #self.output_embedding.weight.data = self.input_embedding.weight.data\n",
        "        \n",
        "        self.input_embedding.weight.data.uniform_(-weight_range, weight_range)\n",
        "        self.fc1.weight.data.uniform_(-weight_range, weight_range)\n",
        "        self.fc1.bias.data.zero_()\n",
        "\n",
        "\n",
        "    # Forward\n",
        "    def forward(self, src):\n",
        "        # Embeddings are fed into the forward layer\n",
        "        embeds = self.input_embedding(src).view(-1, self.d_model * self.ngram_size)\n",
        "        x = self.dropout(self.activation(self.fc1(embeds)))\n",
        "        x = self.output_embedding(x).view(-1, self.vocab_size)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZKaV2NcJCfP"
      },
      "source": [
        "### 3.3 LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dWOzbw-OJCfP"
      },
      "outputs": [],
      "source": [
        "# LSTM Model\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab, words, d_model, d_hidden, n_layers, dropout_rate, seq_len):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Class Parameters\n",
        "        self.vocab = vocab\n",
        "        self.words = words\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.n_layers = n_layers\n",
        "        self.d_hidden = d_hidden\n",
        "        self.d_model = d_model\n",
        "        self.dropout = dropout_rate\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # Embedding Layers\n",
        "        self.input_embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
        "        self.output_embedding = nn.Linear(self.d_model, self.vocab_size)\n",
        "\n",
        "        # Linear Layer\n",
        "        self.fc1 = nn.Linear(seq_len * d_model, self.d_model)\n",
        "\n",
        "        # LSTM Cell\n",
        "        self.lstm = nn.LSTM(input_size=self.d_model, hidden_size=self.d_hidden, num_layers=self.n_layers, dropout=self.dropout)\n",
        "        \n",
        "    # Forward\n",
        "    def forward(self, src, hc):\n",
        "        embeds = self.input_embedding(src).view(-1, self.d_model * self.seq_len)\n",
        "        x = self.fc1(embeds)\n",
        "        preds, hc = self.lstm(x, hc)\n",
        "        preds = self.dropout(preds)\n",
        "        preds = self.output_embedding(preds).view(-1, self.vocab_size)\n",
        "        return [preds, hc]\n",
        "    \n",
        "    def init_weights(self):\n",
        "        pass        \n",
        "    \n",
        "    def detach_hidden(self, hc):\n",
        "        (hidden, cell) = hc\n",
        "        hidden.detach()\n",
        "        cell.detach()\n",
        "        return [hidden, cell]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tphqKoKQJCfQ"
      },
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-gsqaIW7JCfR"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(preds, truth):\n",
        "    preds, truth = np.array(preds.cpu()), np.array(truth.cpu())\n",
        "    acc_arr = preds == truth\n",
        "    return np.sum(acc_arr) / len(acc_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haOXCOXVJCfR"
      },
      "source": [
        "### 4.1 Train One Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "e-p0utxsJCfR"
      },
      "outputs": [],
      "source": [
        "# Train Model Function\n",
        "def train_model(model, train_data_loader, optimizer, criterion):\n",
        "        #print(data.size())\n",
        "        cur_loss = 0\n",
        "        cur_acc = 0\n",
        "        num_batches = len(train_data_loader)\n",
        "        \n",
        "        for batch_idx, (train_features, train_labels) in enumerate(train_data_loader):\n",
        "            optimizer.zero_grad()\n",
        "            # print(f\"Train Features: {train_features[0]}\")\n",
        "            logit_output = model(train_features)\n",
        "            probs = torch.softmax(logit_output, dim=1)\n",
        "            loss = criterion(probs.float(), train_labels.float())\n",
        "            preds = torch.argmax(logit_output, dim=1)\n",
        "            truth = torch.argmax(train_labels, dim=1)\n",
        "            # print(f\"Logit Output = {logit_output}\")\n",
        "            # print(f\"Probs = {probs}; Preds = {preds}\\nLabels = {train_labels}; Truth = {truth}\\n\")\n",
        "\n",
        "            acc = get_accuracy(preds, truth)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            cur_loss += loss.item()\n",
        "            cur_acc += acc\n",
        "\n",
        "        # Shapes of output and targets\n",
        "        # print(f\"Output size: {probs.size()}\")\n",
        "        # print(f\"Target size: {train_labels.size()}\")\n",
        "\n",
        "        # loss = F.mse_loss(output.float(), targets.float())\n",
        "        # output = torch.argmax(output, dim=1)\n",
        "\n",
        "        av_loss = cur_loss / num_batches\n",
        "        av_acc = cur_acc / num_batches\n",
        "        return av_loss, av_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIgIbyPHJCfS"
      },
      "source": [
        "### 4.2 Training over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "69dYxRIcJCfS"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, data_df, epochs, optimizer, criterion):\n",
        "\n",
        "    data = TextDataset(data_df)\n",
        "    train_dataloader = DataLoader(data, batch_size=4000, shuffle=True)\n",
        "    average_loss_list = []\n",
        "    average_perp_list = []\n",
        "    average_acc_list = []\n",
        "    epoch_list = []\n",
        "    \n",
        "    for epoch in range(0, epochs):\n",
        "        model.train(True)\n",
        "        average_loss, average_accuracy = train_model(model, train_dataloader, optimizer, criterion)\n",
        "        average_perplexity = math.exp(average_loss)\n",
        "        model.train(False)\n",
        "\n",
        "        average_loss_list.append(average_loss)\n",
        "        average_perp_list.append(average_perplexity)\n",
        "        average_acc_list.append(average_accuracy)\n",
        "        epoch_list.append(epoch)\n",
        "    \n",
        "        print(f\"At Epoch {epoch}, Accuracy = {average_accuracy} & Loss = {average_loss}\")\n",
        "        print(f\"At Epoch {epoch}, Accuracy = {average_accuracy} & Loss = {average_loss}\")\n",
        "\n",
        "    plot_data(epoch_list, average_loss_list, 'Epoch', 'Loss')\n",
        "    plot_data(epoch_list, average_perp_list, 'Epoch', 'Perplexity')\n",
        "    plot_data(epoch_list, average_acc_list, 'Epoch', 'Accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDbws2hgJCfS"
      },
      "source": [
        "### 4.3 Train n-gram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EGd0YGEXJCfS"
      },
      "outputs": [],
      "source": [
        "def valid_metrics(model, valid_data_loader, ngram_size, vocab_size, criterion):\n",
        "    num_batches = len(valid_data_loader)\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    \n",
        "    for batch_idx, valid_features in enumerate(valid_data_loader):\n",
        "\n",
        "        cur_loss = 0\n",
        "        cur_acc = 0\n",
        "        word_idx = 0\n",
        "\n",
        "        print(f\"Valid Batch Num: {batch_idx}\")\n",
        "        # Sliding Window\n",
        "        for _ in range(len(valid_features[0]) - ngram_size):\n",
        "            # print(f\"Word Idx: {word_idx}\")\n",
        "            start = word_idx\n",
        "            stop = word_idx + ngram_size\n",
        "\n",
        "            # new_features = torch.tensor(list(map(lambda x: torch.select(), train_features)))\n",
        "            new_features = torch.narrow(valid_features, 1, start, ngram_size).to(device)\n",
        "            new_labels = torch.narrow(valid_features, 1, stop, 1).squeeze().to(device)\n",
        "            new_class_labels = []\n",
        "\n",
        "            new_class_labels = F.one_hot(new_labels, vocab_size)\n",
        "            \n",
        "            probs = model(new_features)\n",
        "            pseudo_probs = torch.gather(probs, 1, torch.tensor(new_labels).unsqueeze(1))\n",
        "            pseudo_probs = - torch.log(pseudo_probs) / len(valid_features)\n",
        "        \n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "            truth = torch.argmax(new_class_labels, dim=1)\n",
        "\n",
        "            acc = get_accuracy(preds, truth)\n",
        "            #loss = criterion(probs.float(), new_class_labels.float())\n",
        "            loss = criterion(probs, new_labels)\n",
        "\n",
        "            cur_loss += loss.item()\n",
        "            cur_acc += acc\n",
        "\n",
        "            word_idx += 1\n",
        "\n",
        "        av_loss = cur_loss / word_idx\n",
        "        av_acc = cur_acc / word_idx\n",
        "        print(\"Average Valid Batch Loss\", av_loss)\n",
        "        print(\"Average Valid Batch Accuracy\", av_acc)\n",
        "\n",
        "        total_loss += av_loss\n",
        "        total_acc += av_acc\n",
        "\n",
        "    av_loss = total_loss / num_batches\n",
        "    av_acc = total_acc / num_batches\n",
        "    return av_loss, av_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6lZiCjjOJCfT"
      },
      "outputs": [],
      "source": [
        "# Train Model Function\n",
        "def train_ngram_model(model, train_data_loader, valid_data_loader, optimizer, criterion, ngram_size, vocab_size):\n",
        "    \n",
        "    num_batches = len(train_data_loader)\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "        \n",
        "    for batch_idx, train_features in enumerate(train_data_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        cur_loss = 0\n",
        "        cur_acc = 0\n",
        "        word_idx = 0\n",
        "\n",
        "        print(f\"Batch Num: {batch_idx}\")\n",
        "        # Sliding Window\n",
        "        for _ in range(len(train_features[0]) - ngram_size):\n",
        "            # print(f\"Word Idx: {word_idx}\")\n",
        "            start = word_idx\n",
        "            stop = word_idx + ngram_size\n",
        "\n",
        "            # new_features = torch.tensor(list(map(lambda x: torch.select(), train_features)))\n",
        "            new_features = torch.narrow(train_features, 1, start, ngram_size).to(device)\n",
        "            new_labels = torch.narrow(train_features, 1, stop, 1).squeeze().to(device)\n",
        "            new_class_labels = []\n",
        "\n",
        "            new_class_labels = F.one_hot(new_labels, vocab_size)\n",
        "            \n",
        "            probs = model(new_features)\n",
        "            pseudo_probs = torch.gather(probs, 1, torch.tensor(new_labels).unsqueeze(1))\n",
        "            pseudo_probs = - torch.log(pseudo_probs) / len(train_features)\n",
        "        \n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "            truth = torch.argmax(new_class_labels, dim=1)\n",
        "\n",
        "            acc = get_accuracy(preds, truth)\n",
        "            #loss = criterion(probs.float(), new_class_labels.float())\n",
        "            loss = criterion(probs, new_labels)\n",
        "\n",
        "            cur_loss += loss.item()\n",
        "            cur_acc += acc\n",
        "\n",
        "            word_idx += 1\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        av_loss = cur_loss / word_idx\n",
        "        av_acc = cur_acc / word_idx\n",
        "        print(\"Average Batch Loss\", av_loss)\n",
        "        print(\"Average Batch Accuracy\", av_acc)\n",
        "\n",
        "        total_loss += av_loss\n",
        "        total_acc += av_acc\n",
        "\n",
        "    av_valid_loss, av_valid_acc = valid_metrics(model, valid_data_loader, ngram_size, vocab_size, criterion)\n",
        "\n",
        "    av_loss = total_loss / num_batches\n",
        "    av_acc = total_acc / num_batches\n",
        "    return av_loss, av_acc, av_valid_loss, av_valid_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbcgVupfJCfT"
      },
      "source": [
        "### 4.4 Train n-gram over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Wa5YPE6MJCfT"
      },
      "outputs": [],
      "source": [
        "def train_ngram_loop(model, train_data_df, valid_data_df, epochs, optimizer, criterion, batch_size):\n",
        "\n",
        "    train_data = TextDataset(train_data_df)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    valid_data = TextDataset(valid_data_df)\n",
        "    valid_dataloader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
        "    average_loss_list = []\n",
        "    average_perp_list = []\n",
        "    average_acc_list = []\n",
        "    average_valid_loss_list = []\n",
        "    average_valid_perp_list = []\n",
        "    average_valid_acc_list = []\n",
        "    epoch_list = []\n",
        "    \n",
        "    for epoch in range(0, epochs):\n",
        "        model.train(True)\n",
        "        average_loss, average_accuracy, average_valid_loss, average_valid_accuracy = train_ngram_model(model, train_dataloader, valid_dataloader, optimizer, criterion, model.ngram_size, model.vocab_size)\n",
        "        average_perplexity = math.exp(average_loss)\n",
        "        average_valid_perplexity = math.exp(average_valid_loss)\n",
        "        model.train(False)\n",
        "\n",
        "        average_loss_list.append(average_loss)\n",
        "        average_perp_list.append(average_perplexity)\n",
        "        average_acc_list.append(average_accuracy)\n",
        "        average_valid_loss_list.append(average_valid_loss)\n",
        "        average_valid_perp_list.append(average_valid_perplexity)\n",
        "        average_valid_acc_list.append(average_valid_accuracy)\n",
        "        epoch_list.append(epoch)\n",
        "    \n",
        "        print(f\"At Epoch {epoch}, Accuracy = {average_accuracy} & Loss = {average_loss}\")\n",
        "        print(f\"At Epoch {epoch}, Valid Accuracy = {average_valid_accuracy} & Valid Loss = {average_valid_loss}\")\n",
        "\n",
        "    plot_data(epoch_list, average_loss_list, average_valid_loss_list, 'Epoch', 'Loss')\n",
        "    plot_data(epoch_list, average_perp_list, average_valid_perp_list, 'Epoch', 'Perplexity')\n",
        "    plot_data(epoch_list, average_acc_list, average_valid_acc_list,'Epoch', 'Accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnuocdRWJCfU"
      },
      "source": [
        "### 4.5 Valid Histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cC4iiwlkJCfU"
      },
      "outputs": [],
      "source": [
        "def valid_histogram(model, validation_data, ngram_size, words):\n",
        "    bio_len = 0\n",
        "    bio_probs_real = []\n",
        "    bio_probs_fake = []\n",
        "    total_bio_prob = 0\n",
        "    i = 0\n",
        "\n",
        "    while i < len(validation_data) - (ngram_size+1):\n",
        "        feature = validation_data[i:i+ngram_size].to(device)\n",
        "        label = validation_data[i+ngram_size]\n",
        "        probs = model(feature).squeeze()\n",
        "        label_prob = probs[label]\n",
        "\n",
        "        if i % 50000 == 0: \n",
        "          print(f\"{i}/{len(validation_data)}\")\n",
        "          print(label_prob)\n",
        "\n",
        "        if label_prob == 0:\n",
        "          label_prob += 0.0001\n",
        "\n",
        "        #pseudo_prob = -math.log(label_prob)\n",
        "                \n",
        "        if validation_data[i+ngram_size+1] == words['[REAL]'][0]:\n",
        "           bio_probs_real.append(total_bio_prob / bio_len)\n",
        "           bio_len = 0\n",
        "           total_bio_prob = 0\n",
        "           i += 4\n",
        "        elif validation_data[i+ngram_size+1] == words['[FAKE]'][0]:\n",
        "           bio_probs_fake.append(total_bio_prob / bio_len)\n",
        "           bio_len = 0\n",
        "           total_bio_prob = 0\n",
        "           i += 4\n",
        "        else:\n",
        "            total_bio_prob += label_prob\n",
        "            bio_len += 1\n",
        "            i += 1\n",
        "\n",
        "    #bio_probs_real = list(filter(lambda x: (x > 9.21), bio_probs_real))\n",
        "    #bio_probs_fake = list(filter(lambda x: (x > 9.21), bio_probs_fake))\n",
        "    print(bio_probs_real)\n",
        "    print(bio_probs_fake)\n",
        "    plt.hist(bio_probs_real, label='Reals', alpha=0.5, bins=100)\n",
        "    plt.hist(bio_probs_fake, label='Fakes', alpha=0.5, bins=100)\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xlabel(\"Negative Average Log Probs\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()           "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEWqycTcJCfU"
      },
      "source": [
        "## 5. Testing Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gPQUBXHfJCfU"
      },
      "outputs": [],
      "source": [
        "def test_model(model, df):\n",
        "    data = TextDataset(df)\n",
        "    dataloader = DataLoader(data, batch_size=len(df), shuffle=True)\n",
        "\n",
        "    for batch_idx, (features, labels) in enumerate(dataloader):\n",
        "        print(features.size())\n",
        "        preds = model(features).squeeze()\n",
        "        preds = torch.softmax(preds, dim=1)\n",
        "        preds = torch.argmax(preds, dim=1)\n",
        "        # print(f\"Pre-processed truths: {labels}\")\n",
        "        truth = torch.argmax(labels, dim=1)\n",
        "\n",
        "        # print(f\"preds: {preds}\")\n",
        "        # print(f\"truth: {truth}\")\n",
        "\n",
        "        acc = get_accuracy(truth, preds)\n",
        "        confusion_mat = confusion_matrix(truth, preds)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=[\"FAKE\", \"REAL\"])\n",
        "        disp.plot()\n",
        "        plt.show()\n",
        "        # print(f\"Confusion Matrix : {confusion_mat}\")\n",
        "\n",
        " \n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXeHmTV-JCfV"
      },
      "source": [
        "## 5. Running Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcLWwo6QJCfV"
      },
      "source": [
        "### 5.1 Loading Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4URpjPgYJCfV"
      },
      "outputs": [],
      "source": [
        "class Params:\n",
        "    def __init__(self, **kwargs):\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "            \n",
        "model_map = {0: 'FFNN', 1: 'LSTM', 2: 'FFNN_CLASSIFY', 3: 'LSTM_CLASSIFY'}\n",
        "train_map = {0: 'data/real.train.tok', 1: 'data/fake.train.tok', 2: 'data/mix.train.tok'}\n",
        "valid_map = {0: 'data/real.valid.tok', 1: 'data/fake.valid.tok', 2: 'data/mix.valid.tok'}\n",
        "test_map = {0: 'data/real.test.tok', 1: 'data/fake.test.tok', 2: 'data/mix.test.tok', 3: 'data/blind.test.tok'}\n",
        "\n",
        "model_type = model_map[2]\n",
        "# train_type = [train_map[0], train_map[1]]\n",
        "\n",
        "# Types of data\n",
        "train_type = train_map[2]\n",
        "valid_type = valid_map[2]\n",
        "test_type = test_map[2]\n",
        "\n",
        "args = {\n",
        "    \"d_model\": 32,\n",
        "    \"d_hidden\": 4,\n",
        "    \"n_layers\": 3,\n",
        "    \"batch_size\": 64,\n",
        "    \"seq_len\": 30,\n",
        "    \"printevery\": 5000,\n",
        "    \"window\": 3,\n",
        "    \"epochs\": 5,\n",
        "    \"lr\": 0.0001,\n",
        "    \"dropout\": 0.35,\n",
        "    \"clip\": 2.0,\n",
        "    \"model\": model_type,\n",
        "    \"savename\": model_type.lower(),\n",
        "    \"loadname\": model_type.lower(),\n",
        "    \"trainname\": train_type,\n",
        "    \"validname\": valid_type,\n",
        "    \"testname\": test_type\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V18Xtar5JCfV"
      },
      "source": [
        "### 5.2 Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OyI3xZwFJCfV"
      },
      "outputs": [],
      "source": [
        "# Main Function\n",
        "def main(args): \n",
        "    torch.manual_seed(0)\n",
        "    \n",
        "    params = Params(**args)\n",
        "    train_name = params.trainname\n",
        "    valid_name = params.validname\n",
        "    test_name = params.testname\n",
        "    model_type = params.model\n",
        "    d_mod = params.d_model\n",
        "    d_hid = params.d_hidden\n",
        "    dropout = params.dropout\n",
        "    epochs = params.epochs\n",
        "    window = params.window\n",
        "    batch_size = params.batch_size\n",
        "\n",
        "    # real, fake = params.trainname\n",
        "    # [vocab_real, words_real, train_real] = read_encode(real, [], {}, [], 3)\n",
        "    # [vocab_fake, words_fake, train_fake] = read_encode(fake, [], {}, [], 3)\n",
        "\n",
        "    # train_features = torch.cat((torch.tensor(train_real), torch.tensor(train_fake)))\n",
        "    # train_labels = torch.cat((torch.ones(len(train_real)), torch.zeros(len(train_fake))))\n",
        "    # print(f'train_features: {train_features}')\n",
        "    # print(f'train_labels: {train_labels}')\n",
        "\n",
        "    [train_vocab,train_words,train] = read_encode(train_name,[],{},[],3)\n",
        "    train_data = torch.tensor(train)\n",
        "\n",
        "    [valid_vocab,valid_words,valid] = read_encode(valid_name,[],{},[],3)\n",
        "    valid_data = torch.tensor(valid)\n",
        "    \n",
        "    print('vocab: %d train: %d' % (len(train_vocab),len(train)))\n",
        "    # print(f'vocab: {vocab[10:20]}\\n \\n train: {train[10:20]}')\n",
        "    print(f'fake id: {train_words[\"[FAKE]\"]}')\n",
        "    print(f'real id: {train_words[\"[REAL]\"]}')\n",
        "\n",
        "    # [test_vocab,test_words,test] = read_encode(test_name,vocab,words,[],-1)\n",
        "    [test_vocab,test_words,test] = read_encode(test_name,[],{},[],3)\n",
        "    test_data = torch.tensor(test)\n",
        "\n",
        "    #print('vocab: %d test: %d' % (len(vocab),len(test)))\n",
        "\n",
        "    vocab_size = len(train_vocab)\n",
        "    train_df = process_data(train_data, train_words)\n",
        "    validation_df = process_data(valid_data, valid_words)\n",
        "    test_df = process_data(test_data, test_words)\n",
        "\n",
        "    # print(test_df)\n",
        "    \n",
        "    if model_type == 'FFNN':\n",
        "        ffnn_model = FFNN(train_vocab, train_words, d_mod, d_hid, dropout)\n",
        "        ffnn_model.to(device)\n",
        "        optimizer = torch.optim.SGD(ffnn_model.parameters(), lr=0.01, momentum=0.9)\n",
        "        criterion = nn.BCELoss()\n",
        "        train_loop(ffnn_model, train_df, epochs, optimizer, criterion)\n",
        "        # print(ffnn_model)\n",
        "#          {add code to instantiate the model, train for K epochs and save model to disk}\n",
        "        \n",
        "    if model_type == 'LSTM':\n",
        "        pass\n",
        "#          {add code to instantiate the model, train for K epochs and save model to disk}\n",
        "\n",
        "    if model_type == 'FFNN_CLASSIFY':\n",
        "        # ffnn_model = FFNN(train_vocab, train_words, d_mod, d_hid, dropout)\n",
        "        # ffnn_model.to(device)\n",
        "        # optimizer = torch.optim.SGD(ffnn_model.parameters(), lr=0.01, momentum=0.9)\n",
        "        # criterion = nn.BCELoss()\n",
        "        # train_loop(ffnn_model, train_df, epochs, optimizer, criterion)\n",
        "\n",
        "\n",
        "        # acc = test_model(ffnn_model, validation_df)\n",
        "        # print(f\"Validation Acc: {acc}\")\n",
        "        \n",
        "        # acc = test_model(ffnn_model, test_df)\n",
        "        # print(f\"Test Acc: {acc}\")\n",
        "\n",
        "        padded_data_df = process_ngram(train_data, train_words)\n",
        "        padded_valid_data_df = process_ngram(valid_data, valid_words)\n",
        "        ngram_model = LanguageFFNN(train_vocab, train_words, d_mod, d_hid, dropout, window)\n",
        "        ngram_model.to(device)\n",
        "        optimizer = torch.optim.SGD(ngram_model.parameters(), lr=0.01, momentum=0.9)\n",
        "        criterion = nn.NLLLoss()\n",
        "        train_ngram_loop(ngram_model, padded_data_df, padded_valid_data_df, epochs, optimizer, criterion, batch_size)\n",
        "        #valid_histogram(ngram_model, valid_data, window, valid_words)\n",
        "\n",
        "        torch.save(ngram_model.state_dict(), './ngram_model')\n",
        "#          {add code to instantiate the model, recall model parameters and perform/learn classification}    \n",
        "\n",
        "    if model_type == 'LSTM_CLASSIFY':\n",
        "        pass\n",
        "#          {add code to instantiate the model, recall model parameters and perform/learn classification}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgBde_foJCfW"
      },
      "source": [
        "### 5.3 Running Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7Kt8K9qKJCfW",
        "outputId": "b0ac06da-558b-4467-fac1-c8499b9bdcf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab: 35150 train: 3012820\n",
            "fake id: [122, 3913]\n",
            "real id: [635, 4049]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-04db99e11d97>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  bio_tensor_list.append(torch.tensor(bio).squeeze())\n",
            "<ipython-input-6-ea24f8a37d3b>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data)\n",
            "<ipython-input-6-ea24f8a37d3b>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  padded_data = list(map(torch.tensor, padded_data))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1736\n",
            "torch.Size([1736])\n",
            "606\n",
            "torch.Size([606])\n",
            "Head of Data                                                 Bios\n",
            "0  [tensor(1), tensor(2), tensor(3), tensor(4), t...\n",
            "1  [tensor(0), tensor(29), tensor(0), tensor(29),...\n",
            "2  [tensor(930), tensor(931), tensor(923), tensor...\n",
            "3  [tensor(44), tensor(9), tensor(29), tensor(0),...\n",
            "4  [tensor(44), tensor(1190), tensor(24), tensor(...\n",
            "Head of Data                                                 Bios\n",
            "0  [tensor(1), tensor(2), tensor(3), tensor(4), t...\n",
            "1  [tensor(231), tensor(17), tensor(25), tensor(1...\n",
            "2  [tensor(70), tensor(423), tensor(38), tensor(4...\n",
            "3  [tensor(583), tensor(22), tensor(18), tensor(4...\n",
            "4  [tensor(38), tensor(726), tensor(25), tensor(6...\n",
            "Batch Num: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-4833dde6bca3>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pseudo_probs = torch.gather(probs, 1, torch.tensor(new_labels).unsqueeze(1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Batch Loss 7.855064883510019\n",
            "Average Batch Accuracy 0.05952466820542412\n",
            "Batch Num: 1\n",
            "Average Batch Loss 6.70501851925462\n",
            "Average Batch Accuracy 0.12092469705712637\n",
            "Batch Num: 2\n",
            "Average Batch Loss 6.4182605938765604\n",
            "Average Batch Accuracy 0.14626911425274092\n",
            "Batch Num: 3\n",
            "Average Batch Loss 6.223764106708665\n",
            "Average Batch Accuracy 0.16134412867859205\n",
            "Batch Num: 4\n",
            "Average Batch Loss 6.0776267497441845\n",
            "Average Batch Accuracy 0.17406592613964225\n",
            "Batch Num: 5\n",
            "Average Batch Loss 5.937818755322086\n",
            "Average Batch Accuracy 0.18321732544720137\n",
            "Batch Num: 6\n",
            "Average Batch Loss 5.902758875680121\n",
            "Average Batch Accuracy 0.18425418349682632\n",
            "Batch Num: 7\n",
            "Average Batch Loss 5.810776783700493\n",
            "Average Batch Accuracy 0.1936039382573572\n",
            "Batch Num: 8\n",
            "Average Batch Loss 5.783385487835461\n",
            "Average Batch Accuracy 0.19787759665320254\n",
            "Batch Num: 9\n",
            "Average Batch Loss 5.77778897530346\n",
            "Average Batch Accuracy 0.19641697922677437\n",
            "Batch Num: 10\n",
            "Average Batch Loss 5.6484723267222305\n",
            "Average Batch Accuracy 0.2021242065781881\n",
            "Batch Num: 11\n",
            "Average Batch Loss 5.651740675913551\n",
            "Average Batch Accuracy 0.202016012694749\n",
            "Batch Num: 12\n",
            "Average Batch Loss 5.58340630418499\n",
            "Average Batch Accuracy 0.20631671956145411\n",
            "Batch Num: 13\n",
            "Average Batch Loss 5.632983191055985\n",
            "Average Batch Accuracy 0.2056855885747259\n",
            "Batch Num: 14\n",
            "Average Batch Loss 5.562377442669745\n",
            "Average Batch Accuracy 0.2078404500865551\n",
            "Batch Num: 15\n",
            "Average Batch Loss 5.502106026975804\n",
            "Average Batch Accuracy 0.2125378678592037\n",
            "Batch Num: 16\n",
            "Average Batch Loss 5.501017876738974\n",
            "Average Batch Accuracy 0.2106354587420658\n",
            "Batch Num: 17\n",
            "Average Batch Loss 5.52956357913009\n",
            "Average Batch Accuracy 0.20853469417195614\n",
            "Batch Num: 18\n",
            "Average Batch Loss 5.479458960232127\n",
            "Average Batch Accuracy 0.21248377091748413\n",
            "Batch Num: 19\n",
            "Average Batch Loss 5.477568605767463\n",
            "Average Batch Accuracy 0.21460256780150028\n",
            "Batch Num: 20\n",
            "Average Batch Loss 5.456029609937717\n",
            "Average Batch Accuracy 0.2141247114829775\n",
            "Batch Num: 21\n",
            "Average Batch Loss 5.424191353572839\n",
            "Average Batch Accuracy 0.21947129255626083\n",
            "Batch Num: 22\n",
            "Average Batch Loss 5.394285968735392\n",
            "Average Batch Accuracy 0.2169377524523947\n",
            "Batch Num: 23\n",
            "Average Batch Loss 5.38075299683982\n",
            "Average Batch Accuracy 0.22086879688401614\n",
            "Batch Num: 24\n",
            "Average Batch Loss 5.403882416561719\n",
            "Average Batch Accuracy 0.21919179169070976\n",
            "Batch Num: 25\n",
            "Average Batch Loss 5.330751034268326\n",
            "Average Batch Accuracy 0.22311381996537796\n",
            "Batch Num: 26\n",
            "Average Batch Loss 5.314758455278691\n",
            "Average Batch Accuracy 0.22441214656664743\n",
            "Batch Num: 27\n",
            "Average Batch Loss 5.319101202164643\n",
            "Average Batch Accuracy 0.2128534333525678\n",
            "Valid Batch Num: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-e04b155bae11>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pseudo_probs = torch.gather(probs, 1, torch.tensor(new_labels).unsqueeze(1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Valid Batch Loss 9.730970716397364\n",
            "Average Valid Batch Accuracy 0.03070584577114428\n",
            "Valid Batch Num: 1\n",
            "Average Valid Batch Loss 9.71434594782233\n",
            "Average Valid Batch Accuracy 0.02824419568822554\n",
            "Valid Batch Num: 2\n",
            "Average Valid Batch Loss 9.721640098945024\n",
            "Average Valid Batch Accuracy 0.02933250414593698\n",
            "Valid Batch Num: 3\n",
            "Average Valid Batch Loss 9.676166257652675\n",
            "Average Valid Batch Accuracy 0.0308613184079602\n",
            "Valid Batch Num: 4\n",
            "Average Valid Batch Loss 9.704984362841047\n",
            "Average Valid Batch Accuracy 0.02956571310116086\n",
            "Valid Batch Num: 5\n",
            "Average Valid Batch Loss 9.716773208694079\n",
            "Average Valid Batch Accuracy 0.02920294361525705\n",
            "Valid Batch Num: 6\n",
            "Average Valid Batch Loss 9.728261980844374\n",
            "Average Valid Batch Accuracy 0.028347844112769485\n",
            "Valid Batch Num: 7\n",
            "Average Valid Batch Loss 9.70006878063651\n",
            "Average Valid Batch Accuracy 0.02878834991708126\n",
            "Valid Batch Num: 8\n",
            "Average Valid Batch Loss 9.709328800093871\n",
            "Average Valid Batch Accuracy 0.029643449419568823\n",
            "Valid Batch Num: 9\n",
            "Average Valid Batch Loss 9.631305194810453\n",
            "Average Valid Batch Accuracy 0.031785516860143734\n",
            "At Epoch 0, Accuracy = 0.19468748711977577 & Loss = 5.753025419917348\n",
            "At Epoch 0, Valid Accuracy = 0.02964776810392482 & Valid Loss = 9.703384534873774\n",
            "Batch Num: 0\n",
            "Average Batch Loss 5.3465396624662525\n",
            "Average Batch Accuracy 0.22349249855741488\n",
            "Batch Num: 1\n",
            "Average Batch Loss 5.261134287008163\n",
            "Average Batch Accuracy 0.2316791690709752\n",
            "Batch Num: 2\n",
            "Average Batch Loss 5.352450939663834\n",
            "Average Batch Accuracy 0.22091387766878245\n",
            "Batch Num: 3\n",
            "Average Batch Loss 5.261044333590569\n",
            "Average Batch Accuracy 0.2281177870744374\n",
            "Batch Num: 4\n",
            "Average Batch Loss 5.316746797578155\n",
            "Average Batch Accuracy 0.22177041257934219\n",
            "Batch Num: 5\n",
            "Average Batch Loss 5.314067341451761\n",
            "Average Batch Accuracy 0.22272612521638777\n",
            "Batch Num: 6\n",
            "Average Batch Loss 5.278307670124404\n",
            "Average Batch Accuracy 0.22864974033467975\n",
            "Batch Num: 7\n",
            "Average Batch Loss 5.244502534772781\n",
            "Average Batch Accuracy 0.2295423398730525\n",
            "Batch Num: 8\n",
            "Average Batch Loss 5.2109751142797895\n",
            "Average Batch Accuracy 0.23048002019619157\n",
            "Batch Num: 9\n",
            "Average Batch Loss 5.296622019865165\n",
            "Average Batch Accuracy 0.22848744950952107\n",
            "Batch Num: 10\n",
            "Average Batch Loss 5.2711857716196615\n",
            "Average Batch Accuracy 0.22762189844200809\n",
            "Batch Num: 11\n",
            "Average Batch Loss 5.252681688841409\n",
            "Average Batch Accuracy 0.22427690421234853\n",
            "Batch Num: 12\n",
            "Average Batch Loss 5.232924578441613\n",
            "Average Batch Accuracy 0.23143573283323715\n",
            "Batch Num: 13\n",
            "Average Batch Loss 5.1962686841087535\n",
            "Average Batch Accuracy 0.23136360357761107\n",
            "Batch Num: 14\n",
            "Average Batch Loss 5.206260883938833\n",
            "Average Batch Accuracy 0.22851449798038084\n",
            "Batch Num: 15\n",
            "Average Batch Loss 5.232716720501811\n",
            "Average Batch Accuracy 0.2285415464512406\n",
            "Batch Num: 16\n",
            "Average Batch Loss 5.2107625621216735\n",
            "Average Batch Accuracy 0.23001118003462204\n",
            "Batch Num: 17\n",
            "Average Batch Loss 5.224971129862614\n",
            "Average Batch Accuracy 0.22965053375649164\n",
            "Batch Num: 18\n",
            "Average Batch Loss 5.12490638074198\n",
            "Average Batch Accuracy 0.23571840738603578\n",
            "Batch Num: 19\n",
            "Average Batch Loss 5.2056574624739005\n",
            "Average Batch Accuracy 0.23107508655510675\n",
            "Batch Num: 20\n",
            "Average Batch Loss 5.199852535783798\n",
            "Average Batch Accuracy 0.23065132717830353\n",
            "Batch Num: 21\n",
            "Average Batch Loss 5.193354436034628\n",
            "Average Batch Accuracy 0.2324906231967686\n",
            "Batch Num: 22\n",
            "Average Batch Loss 5.20025884823103\n",
            "Average Batch Accuracy 0.22973167916907097\n",
            "Batch Num: 23\n",
            "Average Batch Loss 5.114408857755006\n",
            "Average Batch Accuracy 0.23688149163300634\n",
            "Batch Num: 24\n",
            "Average Batch Loss 5.22655984080051\n",
            "Average Batch Accuracy 0.2292989036353145\n",
            "Batch Num: 25\n",
            "Average Batch Loss 5.16741601731553\n",
            "Average Batch Accuracy 0.23378894979803808\n",
            "Batch Num: 26\n",
            "Average Batch Loss 5.130095367409629\n",
            "Average Batch Accuracy 0.2363946191575303\n",
            "Batch Num: 27\n",
            "Average Batch Loss 5.338655710633064\n",
            "Average Batch Accuracy 0.21076168493941141\n",
            "Valid Batch Num: 0\n",
            "Average Valid Batch Loss 10.318942032050138\n",
            "Average Valid Batch Accuracy 0.02269900497512438\n",
            "Valid Batch Num: 1\n",
            "Average Valid Batch Loss 10.342536453386247\n",
            "Average Valid Batch Accuracy 0.02308768656716418\n",
            "Valid Batch Num: 2\n",
            "Average Valid Batch Loss 10.43989094849645\n",
            "Average Valid Batch Accuracy 0.020159618573797677\n",
            "Valid Batch Num: 3\n",
            "Average Valid Batch Loss 10.420146353802277\n",
            "Average Valid Batch Accuracy 0.022154850746268658\n",
            "Valid Batch Num: 4\n",
            "Average Valid Batch Loss 10.34357638540948\n",
            "Average Valid Batch Accuracy 0.021999378109452735\n",
            "Valid Batch Num: 5\n",
            "Average Valid Batch Loss 10.439263116067915\n",
            "Average Valid Batch Accuracy 0.020755597014925374\n",
            "Valid Batch Num: 6\n",
            "Average Valid Batch Loss 10.411437169038637\n",
            "Average Valid Batch Accuracy 0.02158478441127695\n",
            "Valid Batch Num: 7\n",
            "Average Valid Batch Loss 10.40194509199405\n",
            "Average Valid Batch Accuracy 0.021662520729684907\n",
            "Valid Batch Num: 8\n",
            "Average Valid Batch Loss 10.443928636326323\n",
            "Average Valid Batch Accuracy 0.021455223880597014\n",
            "Valid Batch Num: 9\n",
            "Average Valid Batch Loss 10.413485973035518\n",
            "Average Valid Batch Accuracy 0.021558872305140912\n",
            "At Epoch 1, Accuracy = 0.22871671750061826 & Loss = 5.236118863479155\n",
            "At Epoch 1, Valid Accuracy = 0.02171175373134328 & Valid Loss = 10.397515215960704\n",
            "Batch Num: 0\n",
            "Average Batch Loss 5.200710686245063\n",
            "Average Batch Accuracy 0.22787435083669935\n",
            "Batch Num: 1\n",
            "Average Batch Loss 5.119560129987802\n",
            "Average Batch Accuracy 0.23461843623773804\n",
            "Batch Num: 2\n",
            "Average Batch Loss 5.136964731111873\n",
            "Average Batch Accuracy 0.23452827466820542\n",
            "Batch Num: 3\n",
            "Average Batch Loss 5.160292760388946\n",
            "Average Batch Accuracy 0.23408648297749568\n",
            "Batch Num: 4\n",
            "Average Batch Loss 5.126532315932734\n",
            "Average Batch Accuracy 0.23583561742642817\n",
            "Batch Num: 5\n",
            "Average Batch Loss 5.101999759398921\n",
            "Average Batch Accuracy 0.23579053664166186\n",
            "Batch Num: 6\n",
            "Average Batch Loss 5.044613925006028\n",
            "Average Batch Accuracy 0.24257970282746683\n",
            "Batch Num: 7\n",
            "Average Batch Loss 5.087364232959736\n",
            "Average Batch Accuracy 0.23607905366416618\n",
            "Batch Num: 8\n",
            "Average Batch Loss 5.159817012141911\n",
            "Average Batch Accuracy 0.23436598384304674\n",
            "Batch Num: 9\n",
            "Average Batch Loss 5.094833560713577\n",
            "Average Batch Accuracy 0.23815276976341604\n",
            "Batch Num: 10\n",
            "Average Batch Loss 5.090153925142327\n",
            "Average Batch Accuracy 0.23906340161569534\n",
            "Batch Num: 11\n",
            "Average Batch Loss 5.130353417619418\n",
            "Average Batch Accuracy 0.2352585833814195\n",
            "Batch Num: 12\n",
            "Average Batch Loss 5.1028162380621325\n",
            "Average Batch Accuracy 0.2369175562608194\n",
            "Batch Num: 13\n",
            "Average Batch Loss 5.080395974386516\n",
            "Average Batch Accuracy 0.23840522215810733\n",
            "Batch Num: 14\n",
            "Average Batch Loss 5.030327941390454\n",
            "Average Batch Accuracy 0.2409748268897865\n",
            "Batch Num: 15\n",
            "Average Batch Loss 5.060038154412747\n",
            "Average Batch Accuracy 0.2399560011540681\n",
            "Batch Num: 16\n",
            "Average Batch Loss 5.065633044515259\n",
            "Average Batch Accuracy 0.24005517888055394\n",
            "Batch Num: 17\n",
            "Average Batch Loss 5.077862339493036\n",
            "Average Batch Accuracy 0.2378191719561454\n",
            "Batch Num: 18\n",
            "Average Batch Loss 5.106383624547352\n",
            "Average Batch Accuracy 0.23749459030582803\n",
            "Batch Num: 19\n",
            "Average Batch Loss 5.042494851594981\n",
            "Average Batch Accuracy 0.2424715089440277\n",
            "Batch Num: 20\n",
            "Average Batch Loss 5.051801113331173\n",
            "Average Batch Accuracy 0.23989288805539527\n",
            "Batch Num: 21\n",
            "Average Batch Loss 5.122888515536247\n",
            "Average Batch Accuracy 0.23613315060588574\n",
            "Batch Num: 22\n",
            "Average Batch Loss 5.073326749603527\n",
            "Average Batch Accuracy 0.24089368147720716\n",
            "Batch Num: 23\n",
            "Average Batch Loss 5.097631172162566\n",
            "Average Batch Accuracy 0.2361241344489325\n",
            "Batch Num: 24\n",
            "Average Batch Loss 5.089822304296962\n",
            "Average Batch Accuracy 0.23905438545874205\n",
            "Batch Num: 25\n",
            "Average Batch Loss 5.041387953926082\n",
            "Average Batch Accuracy 0.24540175995383728\n",
            "Batch Num: 26\n",
            "Average Batch Loss 5.0930604488947315\n",
            "Average Batch Accuracy 0.23752163877668783\n",
            "Batch Num: 27\n",
            "Average Batch Loss 5.229701158151555\n",
            "Average Batch Accuracy 0.2160992498557415\n",
            "Valid Batch Num: 0\n",
            "Average Valid Batch Loss 10.653063301225602\n",
            "Average Valid Batch Accuracy 0.017749792703150913\n",
            "Valid Batch Num: 1\n",
            "Average Valid Batch Loss 10.688660218347957\n",
            "Average Valid Batch Accuracy 0.01715381426202322\n",
            "Valid Batch Num: 2\n",
            "Average Valid Batch Loss 10.720691809013708\n",
            "Average Valid Batch Accuracy 0.017076077943615257\n",
            "Valid Batch Num: 3\n",
            "Average Valid Batch Loss 10.632127434460084\n",
            "Average Valid Batch Accuracy 0.016842868988391376\n",
            "Valid Batch Num: 4\n",
            "Average Valid Batch Loss 10.675338087192618\n",
            "Average Valid Batch Accuracy 0.017775704809286898\n",
            "Valid Batch Num: 5\n",
            "Average Valid Batch Loss 10.702531678561943\n",
            "Average Valid Batch Accuracy 0.016946517412935323\n",
            "Valid Batch Num: 6\n",
            "Average Valid Batch Loss 10.65136612628032\n",
            "Average Valid Batch Accuracy 0.019071310116086235\n",
            "Valid Batch Num: 7\n",
            "Average Valid Batch Loss 10.707812405739652\n",
            "Average Valid Batch Accuracy 0.017076077943615257\n",
            "Valid Batch Num: 8\n",
            "Average Valid Batch Loss 10.687361598607913\n",
            "Average Valid Batch Accuracy 0.016635572139303483\n",
            "Valid Batch Num: 9\n",
            "Average Valid Batch Loss 10.68041777729395\n",
            "Average Valid Batch Accuracy 0.018242122719734622\n",
            "At Epoch 2, Accuracy = 0.23690886210947154 & Loss = 5.100670287176916\n",
            "At Epoch 2, Valid Accuracy = 0.01745698590381426 & Valid Loss = 10.679937043672375\n",
            "Batch Num: 0\n",
            "Average Batch Loss 5.061622837823867\n",
            "Average Batch Accuracy 0.23998304962492786\n",
            "Batch Num: 1\n",
            "Average Batch Loss 5.0244617950551715\n",
            "Average Batch Accuracy 0.24071335833814195\n",
            "Batch Num: 2\n",
            "Average Batch Loss 5.001386604897906\n",
            "Average Batch Accuracy 0.24137153779572995\n",
            "Batch Num: 3\n",
            "Average Batch Loss 5.004958491894942\n",
            "Average Batch Accuracy 0.24364360934795154\n",
            "Batch Num: 4\n",
            "Average Batch Loss 5.036543638253583\n",
            "Average Batch Accuracy 0.2426247836122331\n",
            "Batch Num: 5\n",
            "Average Batch Loss 5.0359017412853735\n",
            "Average Batch Accuracy 0.24144366705135603\n",
            "Batch Num: 6\n",
            "Average Batch Loss 4.950043736065459\n",
            "Average Batch Accuracy 0.2444460473167917\n",
            "Batch Num: 7\n",
            "Average Batch Loss 4.996364093280843\n",
            "Average Batch Accuracy 0.24420261107905367\n",
            "Batch Num: 8\n",
            "Average Batch Loss 5.064618488026711\n",
            "Average Batch Accuracy 0.2375306549336411\n",
            "Batch Num: 9\n",
            "Average Batch Loss 5.057071090501234\n",
            "Average Batch Accuracy 0.24121826312752453\n",
            "Batch Num: 10\n",
            "Average Batch Loss 5.032165019139346\n",
            "Average Batch Accuracy 0.24327394691286786\n",
            "Batch Num: 11\n",
            "Average Batch Loss 4.967431674991394\n",
            "Average Batch Accuracy 0.24529356607039815\n",
            "Batch Num: 12\n",
            "Average Batch Loss 5.06459278353225\n",
            "Average Batch Accuracy 0.24041582515868437\n",
            "Batch Num: 13\n",
            "Average Batch Loss 5.0310407737329115\n",
            "Average Batch Accuracy 0.24098384304673975\n",
            "Batch Num: 14\n",
            "Average Batch Loss 5.061902886378029\n",
            "Average Batch Accuracy 0.2363044575879977\n",
            "Batch Num: 15\n",
            "Average Batch Loss 5.011980988424909\n",
            "Average Batch Accuracy 0.24230921811886902\n",
            "Batch Num: 16\n",
            "Average Batch Loss 4.891288854865345\n",
            "Average Batch Accuracy 0.24970246682054242\n",
            "Batch Num: 17\n",
            "Average Batch Loss 4.988130634531284\n",
            "Average Batch Accuracy 0.2441034333525678\n",
            "Batch Num: 18\n",
            "Average Batch Loss 4.978632110704663\n",
            "Average Batch Accuracy 0.24354443162146566\n",
            "Batch Num: 19\n",
            "Average Batch Loss 5.013212231632195\n",
            "Average Batch Accuracy 0.24350836699365264\n",
            "Batch Num: 20\n",
            "Average Batch Loss 4.971373995534959\n",
            "Average Batch Accuracy 0.24917051356030007\n",
            "Batch Num: 21\n",
            "Average Batch Loss 5.014842985134488\n",
            "Average Batch Accuracy 0.24094777841892673\n",
            "Batch Num: 22\n",
            "Average Batch Loss 5.005019610284452\n",
            "Average Batch Accuracy 0.24472554818234277\n",
            "Batch Num: 23\n",
            "Average Batch Loss 5.0296135043657655\n",
            "Average Batch Accuracy 0.24093876226197344\n",
            "Batch Num: 24\n",
            "Average Batch Loss 5.0460293016472235\n",
            "Average Batch Accuracy 0.2403436959030583\n",
            "Batch Num: 25\n",
            "Average Batch Loss 5.048413353301993\n",
            "Average Batch Accuracy 0.24117318234275822\n",
            "Batch Num: 26\n",
            "Average Batch Loss 4.978171561951591\n",
            "Average Batch Accuracy 0.2450591459896134\n",
            "Batch Num: 27\n",
            "Average Batch Loss 5.205725526231875\n",
            "Average Batch Accuracy 0.21422388920946336\n",
            "Valid Batch Num: 0\n",
            "Average Valid Batch Loss 10.618055406889907\n",
            "Average Valid Batch Accuracy 0.016609660033167495\n",
            "Valid Batch Num: 1\n",
            "Average Valid Batch Loss 10.692081793030695\n",
            "Average Valid Batch Accuracy 0.01681695688225539\n",
            "Valid Batch Num: 2\n",
            "Average Valid Batch Loss 10.627728356255425\n",
            "Average Valid Batch Accuracy 0.017568407960199005\n",
            "Valid Batch Num: 3\n",
            "Average Valid Batch Loss 10.649823985882659\n",
            "Average Valid Batch Accuracy 0.018760364842454396\n",
            "Valid Batch Num: 4\n",
            "Average Valid Batch Loss 10.590195250946095\n",
            "Average Valid Batch Accuracy 0.017568407960199005\n",
            "Valid Batch Num: 5\n",
            "Average Valid Batch Loss 10.644986660326298\n",
            "Average Valid Batch Accuracy 0.017983001658374794\n",
            "Valid Batch Num: 6\n",
            "Average Valid Batch Loss 10.62303197996731\n",
            "Average Valid Batch Accuracy 0.01842350746268657\n",
            "Valid Batch Num: 7\n",
            "Average Valid Batch Loss 10.650857523701479\n",
            "Average Valid Batch Accuracy 0.02008188225538972\n",
            "Valid Batch Num: 8\n",
            "Average Valid Batch Loss 10.59538860384307\n",
            "Average Valid Batch Accuracy 0.019252694859038143\n",
            "Valid Batch Num: 9\n",
            "Average Valid Batch Loss 10.686175691152291\n",
            "Average Valid Batch Accuracy 0.01669430624654502\n",
            "At Epoch 3, Accuracy = 0.24154284477784196 & Loss = 5.020447868338207\n",
            "At Epoch 3, Valid Accuracy = 0.017975919016030954 & Valid Loss = 10.63783252519952\n",
            "Batch Num: 0\n",
            "Average Batch Loss 5.023543508081717\n",
            "Average Batch Accuracy 0.2382970282746682\n",
            "Batch Num: 1\n",
            "Average Batch Loss 4.936034601957758\n",
            "Average Batch Accuracy 0.2473041690709752\n",
            "Batch Num: 2\n",
            "Average Batch Loss 4.964320207013605\n",
            "Average Batch Accuracy 0.24486980669359493\n",
            "Batch Num: 3\n",
            "Average Batch Loss 4.935043048115734\n",
            "Average Batch Accuracy 0.24677221581073283\n",
            "Batch Num: 4\n",
            "Average Batch Loss 4.970591522129849\n",
            "Average Batch Accuracy 0.24679024812463934\n",
            "Batch Num: 5\n",
            "Average Batch Loss 4.938750507511022\n",
            "Average Batch Accuracy 0.2473222013848817\n",
            "Batch Num: 6\n",
            "Average Batch Loss 4.973940331882228\n",
            "Average Batch Accuracy 0.24549192152336988\n",
            "Batch Num: 7\n",
            "Average Batch Loss 4.942086070380456\n",
            "Average Batch Accuracy 0.24866560877091748\n",
            "Batch Num: 8\n",
            "Average Batch Loss 4.885050286327699\n",
            "Average Batch Accuracy 0.2532999134448932\n",
            "Batch Num: 9\n",
            "Average Batch Loss 4.901745546091105\n",
            "Average Batch Accuracy 0.2488188834391229\n",
            "Batch Num: 10\n",
            "Average Batch Loss 4.9590069880323195\n",
            "Average Batch Accuracy 0.24653779572994808\n",
            "Batch Num: 11\n",
            "Average Batch Loss 4.988981803854421\n",
            "Average Batch Accuracy 0.2437878678592037\n",
            "Batch Num: 12\n",
            "Average Batch Loss 4.937139543301098\n",
            "Average Batch Accuracy 0.24835905943450665\n",
            "Batch Num: 13\n",
            "Average Batch Loss 5.008652644561882\n",
            "Average Batch Accuracy 0.2417412002308136\n",
            "Batch Num: 14\n",
            "Average Batch Loss 5.0074149999602025\n",
            "Average Batch Accuracy 0.2432829630698211\n",
            "Batch Num: 15\n",
            "Average Batch Loss 4.914322911420449\n",
            "Average Batch Accuracy 0.2510278418926717\n",
            "Batch Num: 16\n",
            "Average Batch Loss 4.94026259294632\n",
            "Average Batch Accuracy 0.24542880842469705\n",
            "Batch Num: 17\n",
            "Average Batch Loss 4.955947831402056\n",
            "Average Batch Accuracy 0.24703368436237738\n",
            "Batch Num: 18\n",
            "Average Batch Loss 4.993403896182105\n",
            "Average Batch Accuracy 0.2421559434506636\n",
            "Batch Num: 19\n",
            "Average Batch Loss 4.925886986094683\n",
            "Average Batch Accuracy 0.24930575591459897\n",
            "Batch Num: 20\n",
            "Average Batch Loss 4.959251917378447\n",
            "Average Batch Accuracy 0.24638452106174263\n",
            "Batch Num: 21\n",
            "Average Batch Loss 4.985105891070886\n",
            "Average Batch Accuracy 0.24421162723600692\n",
            "Batch Num: 22\n",
            "Average Batch Loss 4.940904981747433\n",
            "Average Batch Accuracy 0.248476269474899\n",
            "Batch Num: 23\n",
            "Average Batch Loss 4.915749422195532\n",
            "Average Batch Accuracy 0.2485033179457588\n",
            "Batch Num: 24\n",
            "Average Batch Loss 5.031431725860261\n",
            "Average Batch Accuracy 0.24156989324870168\n",
            "Batch Num: 25\n",
            "Average Batch Loss 4.977326570875165\n",
            "Average Batch Accuracy 0.2450320975187536\n",
            "Batch Num: 26\n",
            "Average Batch Loss 4.965987025969165\n",
            "Average Batch Accuracy 0.2453747114829775\n",
            "Batch Num: 27\n",
            "Average Batch Loss 4.961772293419902\n",
            "Average Batch Accuracy 0.2268465089440277\n",
            "Valid Batch Num: 0\n",
            "Average Valid Batch Loss 10.806814274384609\n",
            "Average Valid Batch Accuracy 0.020548300165837478\n",
            "Valid Batch Num: 1\n",
            "Average Valid Batch Loss 10.815256058675535\n",
            "Average Valid Batch Accuracy 0.01896766169154229\n",
            "Valid Batch Num: 2\n",
            "Average Valid Batch Loss 10.832494305536324\n",
            "Average Valid Batch Accuracy 0.01896766169154229\n",
            "Valid Batch Num: 3\n",
            "Average Valid Batch Loss 10.804471900016317\n",
            "Average Valid Batch Accuracy 0.01920087064676617\n",
            "Valid Batch Num: 4\n",
            "Average Valid Batch Loss 10.79032316729797\n",
            "Average Valid Batch Accuracy 0.020366915422885573\n",
            "Valid Batch Num: 5\n",
            "Average Valid Batch Loss 10.804257261614696\n",
            "Average Valid Batch Accuracy 0.020833333333333332\n",
            "Valid Batch Num: 6\n",
            "Average Valid Batch Loss 10.799264809780848\n",
            "Average Valid Batch Accuracy 0.020133706467661692\n",
            "Valid Batch Num: 7\n",
            "Average Valid Batch Loss 10.812285738007148\n",
            "Average Valid Batch Accuracy 0.02062603648424544\n",
            "Valid Batch Num: 8\n",
            "Average Valid Batch Loss 10.843504526128816\n",
            "Average Valid Batch Accuracy 0.0189417495854063\n",
            "Valid Batch Num: 9\n",
            "Average Valid Batch Loss 10.762092606147526\n",
            "Average Valid Batch Accuracy 0.020563847429519024\n",
            "At Epoch 4, Accuracy = 0.24545328085071308 & Loss = 4.958559130562982\n",
            "At Epoch 4, Valid Accuracy = 0.01991500829187396 & Valid Loss = 10.807076464758978\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "main(args)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "91613d3f5ca718a684ff9094a61c3901a8c82e744c47f88fcbc13decf1cdde28"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
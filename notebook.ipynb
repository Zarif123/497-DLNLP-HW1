{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding and decoding\n",
    "def decode(vocab,corpus):\n",
    "    \n",
    "    text = ''\n",
    "    for i in range(len(corpus)):\n",
    "        wID = corpus[i]\n",
    "        text = text + vocab[wID] + ' '\n",
    "    return(text)\n",
    "\n",
    "def encode(words,text):\n",
    "    corpus = []\n",
    "    tokens = text.split(' ')\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            wID = words[t][0]\n",
    "        except:\n",
    "            wID = words['<unk>'][0]\n",
    "        corpus.append(wID)\n",
    "    return(corpus)\n",
    "\n",
    "def read_encode(file_name,vocab,words,corpus,threshold):\n",
    "    \n",
    "    wID = len(vocab)\n",
    "    \n",
    "    if threshold > -1:\n",
    "        with open(file_name,'rt', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                line = line.replace('\\n','')\n",
    "                tokens = line.split(' ')\n",
    "                for t in tokens:\n",
    "                    try:\n",
    "                        elem = words[t]\n",
    "                    except:\n",
    "                        elem = [wID,0]\n",
    "                        vocab.append(t)\n",
    "                        wID = wID + 1\n",
    "                    elem[1] = elem[1] + 1\n",
    "                    words[t] = elem\n",
    "\n",
    "        temp = words\n",
    "        words = {}\n",
    "        vocab = []\n",
    "        wID = 0\n",
    "        words['<unk>'] = [wID,100]\n",
    "        vocab.append('<unk>')\n",
    "        for t in temp:\n",
    "            if temp[t][1] >= threshold:\n",
    "                vocab.append(t)\n",
    "                wID = wID + 1\n",
    "                words[t] = [wID,temp[t][1]]\n",
    "            \n",
    "                    \n",
    "    with open(file_name,'rt', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.replace('\\n','')\n",
    "            tokens = line.split(' ')\n",
    "            for t in tokens:\n",
    "                try:\n",
    "                    wID = words[t][0]\n",
    "                except:\n",
    "                    wID = words['<unk>'][0]\n",
    "                corpus.append(wID)\n",
    "                \n",
    "    return [vocab,words,corpus]\n",
    "\n",
    "def plot_data(x, y1, y2, xlabel, ylabel):\n",
    "    plt.plot(x, y1, 'b')\n",
    "    plt.plot(x, y2, 'r')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data-Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(bios, targets):\n",
    "    zipped_data = list(zip(bios, targets))\n",
    "    text_df = pd.DataFrame(zipped_data, columns=['Bios', 'Labels'])\n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def process_data(data, words):\n",
    "    fake_i = (data == words[\"[FAKE]\"][0])\n",
    "    real_i = (data == words[\"[REAL]\"][0])\n",
    "    target_indices = (fake_i + real_i).nonzero()\n",
    "    num_entries = len(target_indices)\n",
    "    bio_tensor_list = []\n",
    "    target_list = []\n",
    "    entry = 0\n",
    "    start_i = 0\n",
    "\n",
    "    while entry < num_entries:\n",
    "        target_i = target_indices[entry]\n",
    "\n",
    "        # Size of data and targets\n",
    "        #print(f\"Data size: {data[start_i:target_i].size()}\")\n",
    "        #print(f\"Target size: {data[target_i].size()}\")\n",
    "\n",
    "        # Take in a list of tensors and use pad sequence\n",
    "        bio = data[start_i:target_i]\n",
    "        target = [1, 0] if data[target_i] == words[\"[FAKE]\"][0] else [0, 1]\n",
    "\n",
    "        bio_tensor_list.append(torch.tensor(bio).squeeze())\n",
    "        target_list.append(target)\n",
    "\n",
    "        start_i = target_i + 1\n",
    "        entry += 1\n",
    "\n",
    "    # have largest size be 880\n",
    "    pad_bias = torch.ones(880)\n",
    "    bio_tensor_list.append(pad_bias)\n",
    "\n",
    "    padded_bios = torch.t(pad_sequence(bio_tensor_list)).to(device)\n",
    "    padded_bios = padded_bios[:-1]\n",
    "    targets = torch.tensor(target_list).to(device)\n",
    "\n",
    "    # print(\"First bio\", padded_bios[0].size())\n",
    "    # print(\"Second bio\", padded_bios[1].size())\n",
    "    data_df = create_df(padded_bios, targets)\n",
    "    return data_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Pre-process n-gram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ngram(data, words):\n",
    "\n",
    "    # Features\n",
    "    data = torch.tensor(data)\n",
    "    new_len = (math.floor(math.sqrt(len(data))) + 1) ** 2\n",
    "    pad_arr = [words['<unk>'][0] for i in range(new_len - len(data))]\n",
    "    pad_arr = torch.tensor(pad_arr)\n",
    "\n",
    "    padded_data = torch.cat((data, pad_arr), dim=0)\n",
    "    padded_data = torch.reshape(padded_data, (int(math.sqrt(new_len)), int(math.sqrt(new_len))))\n",
    "    padded_data = list(map(torch.tensor, padded_data))\n",
    "    print(len(padded_data))\n",
    "    print(padded_data[0].size())\n",
    "    data_df = pd.DataFrame(list(zip(padded_data)), columns=['Bios'])\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_df):\n",
    "        self.text_df = data_df\n",
    "        print(\"Head of Data\", self.text_df.head())\n",
    "        self.x = self.text_df['Bios']\n",
    "        #self.y = self.text_df['Labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]#, self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward Model\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, vocab, words, d_model, d_hidden, dropout):\n",
    "        super().__init__() \n",
    "    \n",
    "        # Class parameters\n",
    "        self.vocab = vocab\n",
    "        self.words = words\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.d_model = d_model\n",
    "        self.d_hidden = d_hidden\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embeds = nn.Embedding(self.vocab_size, self.d_model)\n",
    "\n",
    "        # Linear Layers\n",
    "        self.fc1 = nn.Linear(880 * d_model, d_hidden)\n",
    "        self.fc2 = nn.Linear(d_hidden, 2)\n",
    "\n",
    "        # Nonlinear Layer\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        # Setting weights\n",
    "        self.init_weights()\n",
    "                \n",
    "    # Initialize weights for foward layer\n",
    "    def init_weights(self):\n",
    "        weight_range = 0.1\n",
    "        self.embeds.weight.data.uniform_(-weight_range, weight_range)\n",
    "        self.fc1.weight.data.uniform_(-weight_range, weight_range)\n",
    "        self.fc1.bias.data.zero_()\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, src):\n",
    "        # Embeddings are fed into the forward layer\n",
    "        embeds = self.embeds(src).view((-1, 880 * self.d_model))\n",
    "        x = self.dropout(self.activation(self.fc1(embeds)))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Language FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward Model\n",
    "class LanguageFFNN(nn.Module):\n",
    "    def __init__(self, vocab, words, d_model, d_hidden, dropout, ngram_size):\n",
    "        super().__init__() \n",
    "    \n",
    "        # Class parameters\n",
    "        self.vocab = vocab\n",
    "        self.words = words\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.d_model = d_model\n",
    "        self.d_hidden = d_hidden\n",
    "        self.ngram_size = ngram_size\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Embedding Layers\n",
    "        self.input_embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        # self.output_embedding =  nn.Embedding(self.vocab_size, self.d_model)\n",
    "        \n",
    "        # self.embeds = nn.Embedding(self.vocab_size, self.d_model)\n",
    "\n",
    "        # Linear Layers\n",
    "        self.fc1 = nn.Linear(ngram_size * d_model, self.d_model)\n",
    "        # self.fc2 = nn.Linear(d_hidden, self.vocab_size)\n",
    "        self.output_embedding = nn.Linear(self.d_model, self.vocab_size)\n",
    "\n",
    "        # Nonlinear Layer\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        # Setting weights\n",
    "        self.init_weights()\n",
    "                \n",
    "    # Initialize weights for foward layer\n",
    "    def init_weights(self):\n",
    "        weight_range = 0.1\n",
    "        # tie embedding layers\n",
    "        self.output_embedding.weight.data = self.input_embedding.weight.data\n",
    "        \n",
    "        self.input_embedding.weight.data.uniform_(-weight_range, weight_range)\n",
    "        self.fc1.weight.data.uniform_(-weight_range, weight_range)\n",
    "        self.fc1.bias.data.zero_()\n",
    "\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, src):\n",
    "        # Embeddings are fed into the forward layer\n",
    "        embeds = self.input_embedding(src).view(-1, self.d_model * self.ngram_size)\n",
    "        x = self.dropout(self.activation(self.fc1(embeds)))\n",
    "        x = self.output_embedding(x).view(-1, self.vocab_size)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab, words, d_model, d_hidden, n_layers, dropout_rate, seq_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Class Parameters\n",
    "        self.vocab = vocab\n",
    "        self.words = words\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.n_layers = n_layers\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout_rate\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Embedding Layers\n",
    "        self.input_embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.output_embedding = nn.Linear(self.d_model, self.vocab_size)\n",
    "\n",
    "        # Linear Layer\n",
    "        self.fc1 = nn.Linear(seq_len * d_model, self.d_model)\n",
    "\n",
    "        # LSTM Cell\n",
    "        self.lstm = nn.LSTM(input_size=self.d_model, hidden_size=self.d_hidden, num_layers=self.n_layers, dropout=self.dropout)\n",
    "        \n",
    "    # Forward\n",
    "    def forward(self, src, hc):\n",
    "        embeds = self.input_embedding(src).view(-1, self.d_model * self.seq_len)\n",
    "        x = self.fc1(embeds)\n",
    "        preds, hc = self.lstm(x, hc)\n",
    "        preds = self.dropout(preds)\n",
    "        preds = self.output_embedding(preds).view(-1, self.vocab_size)\n",
    "        return [preds, hc]\n",
    "    \n",
    "    def init_weights(self):\n",
    "        pass        \n",
    "    \n",
    "    def detach_hidden(self, hc):\n",
    "        (hidden, cell) = hc\n",
    "        hidden.detach()\n",
    "        cell.detach()\n",
    "        return [hidden, cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(preds, truth):\n",
    "    preds, truth = np.array(preds.cpu()), np.array(truth.cpu())\n",
    "    acc_arr = preds == truth\n",
    "    return np.sum(acc_arr) / len(acc_arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model Function\n",
    "def train_model(model, train_data_loader, optimizer, criterion):\n",
    "        #print(data.size())\n",
    "        cur_loss = 0\n",
    "        cur_acc = 0\n",
    "        num_batches = len(train_data_loader)\n",
    "        \n",
    "        for batch_idx, (train_features, train_labels) in enumerate(train_data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # print(f\"Train Features: {train_features[0]}\")\n",
    "            logit_output = model(train_features)\n",
    "            probs = torch.softmax(logit_output, dim=1)\n",
    "            loss = criterion(probs.float(), train_labels.float())\n",
    "            preds = torch.argmax(logit_output, dim=1)\n",
    "            truth = torch.argmax(train_labels, dim=1)\n",
    "            # print(f\"Logit Output = {logit_output}\")\n",
    "            # print(f\"Probs = {probs}; Preds = {preds}\\nLabels = {train_labels}; Truth = {truth}\\n\")\n",
    "\n",
    "            acc = get_accuracy(preds, truth)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cur_loss += loss.item()\n",
    "            cur_acc += acc\n",
    "\n",
    "        # Shapes of output and targets\n",
    "        # print(f\"Output size: {probs.size()}\")\n",
    "        # print(f\"Target size: {train_labels.size()}\")\n",
    "\n",
    "        # loss = F.mse_loss(output.float(), targets.float())\n",
    "        # output = torch.argmax(output, dim=1)\n",
    "\n",
    "        av_loss = cur_loss / num_batches\n",
    "        av_acc = cur_acc / num_batches\n",
    "        return av_loss, av_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, data_df, epochs, optimizer, criterion):\n",
    "\n",
    "    data = TextDataset(data_df)\n",
    "    train_dataloader = DataLoader(data, batch_size=4000, shuffle=True)\n",
    "    average_loss_list = []\n",
    "    average_perp_list = []\n",
    "    average_acc_list = []\n",
    "    epoch_list = []\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        model.train(True)\n",
    "        average_loss, average_accuracy = train_model(model, train_dataloader, optimizer, criterion)\n",
    "        average_perplexity = math.exp(average_loss)\n",
    "        model.train(False)\n",
    "\n",
    "        average_loss_list.append(average_loss)\n",
    "        average_perp_list.append(average_perplexity)\n",
    "        average_acc_list.append(average_accuracy)\n",
    "        epoch_list.append(epoch)\n",
    "    \n",
    "        print(f\"At Epoch {epoch}, Accuracy = {average_accuracy} & Loss = {average_loss}\")\n",
    "        print(f\"At Epoch {epoch}, Accuracy = {average_accuracy} & Loss = {average_loss}\")\n",
    "\n",
    "    plot_data(epoch_list, average_loss_list, 'Epoch', 'Loss')\n",
    "    plot_data(epoch_list, average_perp_list, 'Epoch', 'Perplexity')\n",
    "    plot_data(epoch_list, average_acc_list, 'Epoch', 'Accuracy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train n-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_metrics(model, valid_data_loader, ngram_size, vocab_size, criterion):\n",
    "    num_batches = len(valid_data_loader)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    \n",
    "    for batch_idx, valid_features in enumerate(valid_data_loader):\n",
    "\n",
    "        cur_loss = 0\n",
    "        cur_acc = 0\n",
    "        word_idx = 0\n",
    "\n",
    "        print(f\"Valid Batch Num: {batch_idx}\")\n",
    "        # Sliding Window\n",
    "        for _ in range(len(valid_features[0]) - ngram_size):\n",
    "            # print(f\"Word Idx: {word_idx}\")\n",
    "            start = word_idx\n",
    "            stop = word_idx + ngram_size\n",
    "\n",
    "            # new_features = torch.tensor(list(map(lambda x: torch.select(), train_features)))\n",
    "            new_features = torch.narrow(valid_features, 1, start, ngram_size).to(device)\n",
    "            new_labels = torch.narrow(valid_features, 1, stop, 1).squeeze().to(device)\n",
    "            new_class_labels = []\n",
    "\n",
    "            new_class_labels = F.one_hot(new_labels, vocab_size)\n",
    "            \n",
    "            probs = model(new_features)\n",
    "            pseudo_probs = torch.gather(probs, 1, torch.tensor(new_labels).unsqueeze(1))\n",
    "            pseudo_probs = - torch.log(pseudo_probs) / len(valid_features)\n",
    "        \n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            truth = torch.argmax(new_class_labels, dim=1)\n",
    "\n",
    "            acc = get_accuracy(preds, truth)\n",
    "            loss = criterion(probs.float(), new_class_labels.float())\n",
    "\n",
    "            cur_loss += loss.item()\n",
    "            cur_acc += acc\n",
    "\n",
    "            word_idx += 1\n",
    "\n",
    "        av_loss = cur_loss / word_idx\n",
    "        av_acc = cur_acc / word_idx\n",
    "        print(\"Average Valid Batch Loss\", av_loss)\n",
    "        print(\"Average Valid Batch Accuracy\", av_acc)\n",
    "\n",
    "        total_loss += av_loss\n",
    "        total_acc += av_acc\n",
    "\n",
    "    av_loss = total_loss / num_batches\n",
    "    av_acc = total_acc / num_batches\n",
    "    return av_loss, av_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model Function\n",
    "def train_ngram_model(model, train_data_loader, valid_data_loader, optimizer, criterion, ngram_size, vocab_size):\n",
    "    \n",
    "    num_batches = len(train_data_loader)\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "        \n",
    "    for batch_idx, train_features in enumerate(train_data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cur_loss = 0\n",
    "        cur_acc = 0\n",
    "        word_idx = 0\n",
    "\n",
    "        print(f\"Batch Num: {batch_idx}\")\n",
    "        # Sliding Window\n",
    "        for _ in range(len(train_features[0]) - ngram_size):\n",
    "            # print(f\"Word Idx: {word_idx}\")\n",
    "            start = word_idx\n",
    "            stop = word_idx + ngram_size\n",
    "\n",
    "            # new_features = torch.tensor(list(map(lambda x: torch.select(), train_features)))\n",
    "            new_features = torch.narrow(train_features, 1, start, ngram_size).to(device)\n",
    "            new_labels = torch.narrow(train_features, 1, stop, 1).squeeze().to(device)\n",
    "            new_class_labels = []\n",
    "\n",
    "            new_class_labels = F.one_hot(new_labels, vocab_size)\n",
    "            \n",
    "            probs = model(new_features)\n",
    "            pseudo_probs = torch.gather(probs, 1, torch.tensor(new_labels).unsqueeze(1))\n",
    "            pseudo_probs = - torch.log(pseudo_probs) / len(train_features)\n",
    "        \n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            truth = torch.argmax(new_class_labels, dim=1)\n",
    "\n",
    "            acc = get_accuracy(preds, truth)\n",
    "            loss = criterion(probs.float(), new_class_labels.float())\n",
    "\n",
    "            cur_loss += loss.item()\n",
    "            cur_acc += acc\n",
    "\n",
    "            word_idx += 1\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        av_loss = cur_loss / word_idx\n",
    "        av_acc = cur_acc / word_idx\n",
    "        print(\"Average Batch Loss\", av_loss)\n",
    "        print(\"Average Batch Accuracy\", av_acc)\n",
    "\n",
    "        total_loss += av_loss\n",
    "        total_acc += av_acc\n",
    "\n",
    "    av_valid_loss, av_valid_acc = valid_metrics(model, valid_data_loader, ngram_size, vocab_size, criterion)\n",
    "\n",
    "    av_loss = total_loss / num_batches\n",
    "    av_acc = total_acc / num_batches\n",
    "    return av_loss, av_acc, av_valid_loss, av_valid_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Train n-gram over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_loop(model, train_data_df, valid_data_df, epochs, optimizer, criterion, batch_size):\n",
    "\n",
    "    train_data = TextDataset(train_data_df)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    valid_data = TextDataset(valid_data_df)\n",
    "    valid_dataloader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "    average_loss_list = []\n",
    "    average_perp_list = []\n",
    "    average_acc_list = []\n",
    "    average_valid_loss_list = []\n",
    "    average_valid_perp_list = []\n",
    "    average_valid_acc_list = []\n",
    "    epoch_list = []\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        model.train(True)\n",
    "        average_loss, average_accuracy, average_valid_loss, average_valid_accuracy = train_ngram_model(model, train_dataloader, valid_dataloader, optimizer, criterion, model.ngram_size, model.vocab_size)\n",
    "        average_perplexity = math.exp(average_loss)\n",
    "        average_valid_perplexity = math.exp(average_valid_loss)\n",
    "        model.train(False)\n",
    "\n",
    "        average_loss_list.append(average_loss)\n",
    "        average_perp_list.append(average_perplexity)\n",
    "        average_acc_list.append(average_accuracy)\n",
    "        average_valid_loss_list.append(average_valid_loss)\n",
    "        average_valid_perp_list.append(average_valid_perplexity)\n",
    "        average_valid_acc_list.append(average_valid_accuracy)\n",
    "        epoch_list.append(epoch)\n",
    "    \n",
    "        print(f\"At Epoch {epoch}, Accuracy = {average_accuracy} & Loss = {average_loss}\")\n",
    "        print(f\"At Epoch {epoch}, Valid Accuracy = {average_valid_accuracy} & Valid Loss = {average_valid_loss}\")\n",
    "\n",
    "    plot_data(epoch_list, average_loss_list, average_valid_loss_list, 'Epoch', 'Loss')\n",
    "    plot_data(epoch_list, average_perp_list, average_valid_perp_list, 'Epoch', 'Perplexity')\n",
    "    plot_data(epoch_list, average_acc_list, average_valid_acc_list,'Epoch', 'Accuracy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Valid Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_histogram(model, validation_data, ngram_size, words):\n",
    "    bio_len = 0\n",
    "    bio_probs = []\n",
    "    total_bio_prob = 0\n",
    "    i = 0\n",
    "\n",
    "    while i < len(validation_data) - (ngram_size+1):\n",
    "        feature = validation_data[i:i+ngram_size].to(device)\n",
    "        label = validation_data[i+ngram_size]\n",
    "        probs = model(feature).squeeze()\n",
    "        label_prob = probs[label]\n",
    "\n",
    "        if i % 1000 == 0: \n",
    "          print(f\"{i}/{len(validation_data)}\")\n",
    "          print(label_prob)\n",
    "\n",
    "        if label_prob == 0:\n",
    "          label_prob += 0.0001\n",
    "\n",
    "        pseudo_prob = -math.log(label_prob)\n",
    "                \n",
    "        if validation_data[i+ngram_size+1] == words['[REAL]'] or \\\n",
    "           validation_data[i+ngram_size+1] == words['[FAKE]']:\n",
    "           bio_probs.append(total_bio_prob / bio_len)\n",
    "           bio_len = 0\n",
    "           total_bio_prob = 0\n",
    "           i += 4\n",
    "        else:\n",
    "            total_bio_prob += pseudo_prob\n",
    "            bio_len += 1\n",
    "            i += 1\n",
    "\n",
    "    plt.hist(bio_probs)\n",
    "    plt.xlabel(\"Negative Average Log Probs\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()           "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, df):\n",
    "    data = TextDataset(df)\n",
    "    dataloader = DataLoader(data, batch_size=len(df), shuffle=True)\n",
    "\n",
    "    for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "        print(features.size())\n",
    "        preds = model(features).squeeze()\n",
    "        preds = torch.softmax(preds, dim=1)\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        # print(f\"Pre-processed truths: {labels}\")\n",
    "        truth = torch.argmax(labels, dim=1)\n",
    "\n",
    "        # print(f\"preds: {preds}\")\n",
    "        # print(f\"truth: {truth}\")\n",
    "\n",
    "        acc = get_accuracy(truth, preds)\n",
    "        confusion_mat = confusion_matrix(truth, preds)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=[\"FAKE\", \"REAL\"])\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "        # print(f\"Confusion Matrix : {confusion_mat}\")\n",
    "\n",
    " \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loading Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "model_map = {0: 'FFNN', 1: 'LSTM', 2: 'FFNN_CLASSIFY', 3: 'LSTM_CLASSIFY'}\n",
    "train_map = {0: 'data/real.train.tok', 1: 'data/fake.train.tok', 2: 'data/mix.train.tok'}\n",
    "valid_map = {0: 'data/real.valid.tok', 1: 'data/fake.valid.tok', 2: 'data/mix.valid.tok'}\n",
    "test_map = {0: 'data/real.test.tok', 1: 'data/fake.test.tok', 2: 'data/mix.test.tok', 3: 'data/blind.test.tok'}\n",
    "\n",
    "model_type = model_map[2]\n",
    "# train_type = [train_map[0], train_map[1]]\n",
    "\n",
    "# Types of data\n",
    "train_type = train_map[2]\n",
    "valid_type = valid_map[2]\n",
    "test_type = test_map[2]\n",
    "\n",
    "args = {\n",
    "    \"d_model\": 4,\n",
    "    \"d_hidden\": 4,\n",
    "    \"n_layers\": 3,\n",
    "    \"batch_size\": 64,\n",
    "    \"seq_len\": 30,\n",
    "    \"printevery\": 5000,\n",
    "    \"window\": 3,\n",
    "    \"epochs\": 1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"dropout\": 0.35,\n",
    "    \"clip\": 2.0,\n",
    "    \"model\": model_type,\n",
    "    \"savename\": model_type.lower(),\n",
    "    \"loadname\": model_type.lower(),\n",
    "    \"trainname\": train_type,\n",
    "    \"validname\": valid_type,\n",
    "    \"testname\": test_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function\n",
    "def main(args): \n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    params = Params(**args)\n",
    "    train_name = params.trainname\n",
    "    valid_name = params.validname\n",
    "    test_name = params.testname\n",
    "    model_type = params.model\n",
    "    d_mod = params.d_model\n",
    "    d_hid = params.d_hidden\n",
    "    dropout = params.dropout\n",
    "    epochs = params.epochs\n",
    "    window = params.window\n",
    "    batch_size = params.batch_size\n",
    "\n",
    "    # real, fake = params.trainname\n",
    "    # [vocab_real, words_real, train_real] = read_encode(real, [], {}, [], 3)\n",
    "    # [vocab_fake, words_fake, train_fake] = read_encode(fake, [], {}, [], 3)\n",
    "\n",
    "    # train_features = torch.cat((torch.tensor(train_real), torch.tensor(train_fake)))\n",
    "    # train_labels = torch.cat((torch.ones(len(train_real)), torch.zeros(len(train_fake))))\n",
    "    # print(f'train_features: {train_features}')\n",
    "    # print(f'train_labels: {train_labels}')\n",
    "\n",
    "    [train_vocab,train_words,train] = read_encode(train_name,[],{},[],3)\n",
    "    train_data = torch.tensor(train)\n",
    "\n",
    "    [valid_vocab,valid_words,valid] = read_encode(valid_name,[],{},[],3)\n",
    "    valid_data = torch.tensor(valid)\n",
    "    \n",
    "    print('vocab: %d train: %d' % (len(train_vocab),len(train)))\n",
    "    # print(f'vocab: {vocab[10:20]}\\n \\n train: {train[10:20]}')\n",
    "    print(f'fake id: {train_words[\"[FAKE]\"]}')\n",
    "    print(f'real id: {train_words[\"[REAL]\"]}')\n",
    "\n",
    "    # [test_vocab,test_words,test] = read_encode(test_name,vocab,words,[],-1)\n",
    "    [test_vocab,test_words,test] = read_encode(test_name,[],{},[],3)\n",
    "    test_data = torch.tensor(test)\n",
    "\n",
    "    #print('vocab: %d test: %d' % (len(vocab),len(test)))\n",
    "\n",
    "    vocab_size = len(train_vocab)\n",
    "    train_df = process_data(train_data, train_words)\n",
    "    validation_df = process_data(valid_data, valid_words)\n",
    "    test_df = process_data(test_data, test_words)\n",
    "\n",
    "    # print(test_df)\n",
    "    \n",
    "    if model_type == 'FFNN':\n",
    "        ffnn_model = FFNN(train_vocab, train_words, d_mod, d_hid, dropout)\n",
    "        ffnn_model.to(device)\n",
    "        optimizer = torch.optim.SGD(ffnn_model.parameters(), lr=0.01, momentum=0.9)\n",
    "        criterion = nn.BCELoss()\n",
    "        train_loop(ffnn_model, train_df, epochs, optimizer, criterion)\n",
    "        # print(ffnn_model)\n",
    "#          {add code to instantiate the model, train for K epochs and save model to disk}\n",
    "        \n",
    "    if model_type == 'LSTM':\n",
    "        pass\n",
    "#          {add code to instantiate the model, train for K epochs and save model to disk}\n",
    "\n",
    "    if model_type == 'FFNN_CLASSIFY':\n",
    "        # ffnn_model = FFNN(train_vocab, train_words, d_mod, d_hid, dropout)\n",
    "        # ffnn_model.to(device)\n",
    "        # optimizer = torch.optim.SGD(ffnn_model.parameters(), lr=0.01, momentum=0.9)\n",
    "        # criterion = nn.BCELoss()\n",
    "        # train_loop(ffnn_model, train_df, epochs, optimizer, criterion)\n",
    "\n",
    "\n",
    "        # acc = test_model(ffnn_model, validation_df)\n",
    "        # print(f\"Validation Acc: {acc}\")\n",
    "        \n",
    "        # acc = test_model(ffnn_model, test_df)\n",
    "        # print(f\"Test Acc: {acc}\")\n",
    "\n",
    "        padded_data_df = process_ngram(train_data, train_words)\n",
    "        padded_valid_data_df = process_ngram(valid_data, valid_words)\n",
    "        ngram_model = LanguageFFNN(train_vocab, train_words, d_mod, d_hid, dropout, window)\n",
    "        ngram_model.to(device)\n",
    "        optimizer = torch.optim.SGD(ngram_model.parameters(), lr=0.01, momentum=0.9)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_ngram_loop(ngram_model, padded_data_df, padded_valid_data_df, epochs, optimizer, criterion, batch_size)\n",
    "        valid_histogram(ngram_model, valid_data, window, valid_words)\n",
    "\n",
    "        torch.save(ngram_model.state_dict(), './ngram_model')\n",
    "#          {add code to instantiate the model, recall model parameters and perform/learn classification}    \n",
    "\n",
    "    if model_type == 'LSTM_CLASSIFY':\n",
    "        pass\n",
    "#          {add code to instantiate the model, recall model parameters and perform/learn classification}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Running Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 35150 train: 3012820\n",
      "fake id: [122, 3913]\n",
      "real id: [635, 4049]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwny1\\AppData\\Local\\Temp/ipykernel_16860/2028812222.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bio_tensor_list.append(torch.tensor(bio).squeeze())\n",
      "C:\\Users\\jwny1\\AppData\\Local\\Temp/ipykernel_16860/2028812222.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bio_tensor_list.append(torch.tensor(bio).squeeze())\n",
      "C:\\Users\\jwny1\\AppData\\Local\\Temp/ipykernel_16860/2028812222.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bio_tensor_list.append(torch.tensor(bio).squeeze())\n",
      "C:\\Users\\jwny1\\AppData\\Local\\Temp/ipykernel_16860/2894492633.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data)\n",
      "C:\\Users\\jwny1\\AppData\\Local\\Temp/ipykernel_16860/2894492633.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_data = list(map(torch.tensor, padded_data))\n",
      "C:\\Users\\jwny1\\AppData\\Local\\Temp/ipykernel_16860/2894492633.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1736\n",
      "torch.Size([1736])\n",
      "606\n",
      "torch.Size([606])\n",
      "Head of Data                                                 Bios\n",
      "0  [tensor(1), tensor(2), tensor(3), tensor(4), t...\n",
      "1  [tensor(0), tensor(29), tensor(0), tensor(29),...\n",
      "2  [tensor(930), tensor(931), tensor(923), tensor...\n",
      "3  [tensor(44), tensor(9), tensor(29), tensor(0),...\n",
      "4  [tensor(44), tensor(1190), tensor(24), tensor(...\n",
      "Head of Data                                                 Bios\n",
      "0  [tensor(1), tensor(2), tensor(3), tensor(4), t...\n",
      "1  [tensor(231), tensor(17), tensor(25), tensor(1...\n",
      "2  [tensor(70), tensor(423), tensor(38), tensor(4...\n",
      "3  [tensor(583), tensor(22), tensor(18), tensor(4...\n",
      "4  [tensor(38), tensor(726), tensor(25), tensor(6...\n",
      "Batch Num: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwny1\\AppData\\Local\\Temp/ipykernel_16860/2894492633.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_data = list(map(torch.tensor, padded_data))\n",
      "C:\\Users\\jwny1\\AppData\\Local\\Temp/ipykernel_16860/4010859995.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pseudo_probs = torch.gather(probs, 1, torch.tensor(new_labels).unsqueeze(1))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16860/1730160679.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16860/1177002522.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mtrain_ngram_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_data_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_valid_data_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mvalid_histogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16860/1577324365.py\u001b[0m in \u001b[0;36mtrain_ngram_loop\u001b[1;34m(model, train_data_df, valid_data_df, epochs, optimizer, criterion, batch_size)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0maverage_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_valid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_valid_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_ngram_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mngram_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0maverage_perplexity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0maverage_valid_perplexity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_valid_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16860/4010859995.py\u001b[0m in \u001b[0;36mtrain_ngram_model\u001b[1;34m(model, train_data_loader, valid_data_loader, optimizer, criterion, ngram_size, vocab_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_class_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mcur_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jwny1\\miniconda3\\envs\\hw6-nn\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jwny1\\miniconda3\\envs\\hw6-nn\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[1;32mc:\\Users\\jwny1\\miniconda3\\envs\\hw6-nn\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2844\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2846\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91613d3f5ca718a684ff9094a61c3901a8c82e744c47f88fcbc13decf1cdde28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

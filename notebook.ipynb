{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy Contents to starter.py for submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding and decoding\n",
    "def decode(vocab,corpus):\n",
    "    \n",
    "    text = ''\n",
    "    for i in range(len(corpus)):\n",
    "        wID = corpus[i]\n",
    "        text = text + vocab[wID] + ' '\n",
    "    return(text)\n",
    "\n",
    "def encode(words,text):\n",
    "    corpus = []\n",
    "    tokens = text.split(' ')\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            wID = words[t][0]\n",
    "        except:\n",
    "            wID = words['<unk>'][0]\n",
    "        corpus.append(wID)\n",
    "    return(corpus)\n",
    "\n",
    "def read_encode(file_name,vocab,words,corpus,threshold):\n",
    "    \n",
    "    wID = len(vocab)\n",
    "    \n",
    "    if threshold > -1:\n",
    "        with open(file_name,'rt', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                line = line.replace('\\n','')\n",
    "                tokens = line.split(' ')\n",
    "                for t in tokens:\n",
    "                    try:\n",
    "                        elem = words[t]\n",
    "                    except:\n",
    "                        elem = [wID,0]\n",
    "                        vocab.append(t)\n",
    "                        wID = wID + 1\n",
    "                    elem[1] = elem[1] + 1\n",
    "                    words[t] = elem\n",
    "\n",
    "        temp = words\n",
    "        words = {}\n",
    "        vocab = []\n",
    "        wID = 0\n",
    "        words['<unk>'] = [wID,100]\n",
    "        vocab.append('<unk>')\n",
    "        for t in temp:\n",
    "            if temp[t][1] >= threshold:\n",
    "                vocab.append(t)\n",
    "                wID = wID + 1\n",
    "                words[t] = [wID,temp[t][1]]\n",
    "            \n",
    "                    \n",
    "    with open(file_name,'rt', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.replace('\\n','')\n",
    "            tokens = line.split(' ')\n",
    "            for t in tokens:\n",
    "                try:\n",
    "                    wID = words[t][0]\n",
    "                except:\n",
    "                    wID = words['<unk>'][0]\n",
    "                corpus.append(wID)\n",
    "                \n",
    "    return [vocab,words,corpus]\n",
    "\n",
    "def plot_data(x, y, xlabel, ylabel):\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(bios, targets, name):\n",
    "    zipped_data = list(zip(bios, targets))\n",
    "    text_df = pd.DataFrame(zipped_data, columns=['Bios', 'Labels'])\n",
    "    text_df.to_pickle(f'data/{name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def process_data(data, train_name):\n",
    "    fake_i = (data == 122)\n",
    "    real_i = (data == 635)\n",
    "    target_indices = (fake_i + real_i).nonzero()\n",
    "    num_entries = len(target_indices)\n",
    "    bio_tensor_list = []\n",
    "    target_list = []\n",
    "    entry = 0\n",
    "    start_i = 0\n",
    "\n",
    "    while entry < num_entries:\n",
    "        target_i = target_indices[entry]\n",
    "        # Size of data and targets\n",
    "        #print(f\"Data size: {data[start_i:target_i].size()}\")\n",
    "        #print(f\"Target size: {data[target_i].size()}\")\n",
    "\n",
    "        # Take in a list of tensors and use pad sequence\n",
    "        bio = data[start_i:target_i]\n",
    "        target = [1, 0] if data[target_i] == 122 else [0, 1]\n",
    "        # target = data[target_i]\n",
    "\n",
    "        bio_tensor_list.append(torch.tensor(bio).squeeze())\n",
    "        target_list.append(target)\n",
    "\n",
    "        start_i = target_i + 1\n",
    "        entry += 1\n",
    "\n",
    "    padded_bios = torch.t(pad_sequence(bio_tensor_list))\n",
    "    targets = torch.tensor(target_list)\n",
    "    print(\"First bio\", padded_bios[0].size())\n",
    "    print(\"Second bio\", padded_bios[1].size())\n",
    "    create_df(padded_bios, targets, train_name)\n",
    "    return padded_bios, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_name):\n",
    "        text_df = pd.read_csv(file_name)\n",
    "\n",
    "        self.x = text_df.bios\n",
    "        self.y = text_df.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# text_dataset = TextDataset(\"/path\")\n",
    "# train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward Model\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, vocab, words, d_model, d_hidden, dropout):\n",
    "        super().__init__() \n",
    "    \n",
    "        # Class parameters\n",
    "        self.vocab = vocab\n",
    "        self.words = words\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.d_model = d_model\n",
    "        self.d_hidden = d_hidden\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embeds = nn.Embedding(self.vocab_size, self.d_model)\n",
    "\n",
    "        # Linear Layers\n",
    "        self.fc1 = nn.Linear(880 * d_model, d_hidden)\n",
    "        self.fc2 = nn.Linear(d_hidden, 2)\n",
    "\n",
    "        # Nonlinear Layer\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        # Setting weights\n",
    "        self.init_weights()\n",
    "                \n",
    "    # Initialize weights for foward layer\n",
    "    def init_weights(self):\n",
    "        weight_range = 0.1\n",
    "        self.embeds.weight.data.uniform_(-weight_range, weight_range)\n",
    "        self.fc1.weight.data.uniform_(-weight_range, weight_range)\n",
    "        self.fc1.bias.data.zero_()\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, src):\n",
    "        # Embeddings are fed into the forward layer\n",
    "        embeds = self.embeds(src).view((-1, 880 * self.d_model))\n",
    "        x = self.activation(self.fc1(embeds))\n",
    "        x = self.fc2(x)\n",
    "        # probs = nn.Softmax(x)\n",
    "        # print(\"probs\", probs)\n",
    "        # return probs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,vocab,words,d_model,d_hidden,n_layers,dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Class Parameters\n",
    "        self.vocab = vocab\n",
    "        self.words = words\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.n_layers = n_layers\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embeds = nn.Embedding(self.vocab_size,self.d_model)\n",
    "        \n",
    "    # Forward\n",
    "    def forward(self,src,h):\n",
    "        embeds = self.dropout(self.embeds(src))       \n",
    "        return [preds,h]\n",
    "    \n",
    "    def init_weights(self):\n",
    "        pass        \n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        return [hidden, cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model Function\n",
    "def train_model(model, data, targets, optimizer):\n",
    "        print(data.size())\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit_output = model(data).squeeze()\n",
    "        # probs = torch.softmax(logit_output, dim=1)\n",
    "\n",
    "        # Shapes of output and targets\n",
    "        # print(f\"Output size: {probs.size()}\")\n",
    "        print(f\"Output size: {logit_output.size()}\")\n",
    "        print(f\"Target size: {targets.size()}\")\n",
    "\n",
    "        # loss = F.mse_loss(output.float(), targets.float())\n",
    "        # output = torch.argmax(output, dim=1)\n",
    "        loss = F.binary_cross_entropy(logit_output.float(), targets.float())\n",
    "        print(\"Loss:\", loss)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "def train_loop(model, features, targets, epochs, optimizer):\n",
    "    losses = []\n",
    "    epoch_list = []\n",
    "    perplexities = []\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        # for batch_idx, (train_features, train_labels) in enumerate(train_dataloader):\n",
    "            \n",
    "        # INDENT\n",
    "        loss = train_model(model, features, targets, optimizer)\n",
    "        print(epoch)\n",
    "        epoch_list.append(epoch)\n",
    "\n",
    "             # if batch_idx % 10:\n",
    "            \n",
    "\n",
    "        # INDENT TWICE\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        perplexity = torch.exp(loss)\n",
    "        perplexities.append(perplexity.item())\n",
    "    plot_data(epoch_list, losses, 'Epoch', 'Loss')\n",
    "    plot_data(epoch_list, perplexities, 'Epoch', 'Perplexity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {0: 'FFNN', 1: 'LSTM', 2: 'FFNN_CLASSIFY', 3: 'LSTM_CLASSIFY'}\n",
    "train_map = {0: 'data/real.train.tok', 1: 'data/fake.train.tok', 2: 'data/mix.train.tok'}\n",
    "valid_map = {0: 'data/real.valid.tok', 1: 'data/fake.valid.tok', 2: 'data/mix.valid.tok'}\n",
    "test_map = {0: 'data/real.test.tok', 1: 'data/fake.test.tok', 2: 'data/mix.test.tok', 3: 'data/blind.test.tok'}\n",
    "\n",
    "model_type = model_map[0]\n",
    "# train_type = [train_map[0], train_map[1]]\n",
    "\n",
    "# Types of data\n",
    "train_type = train_map[2]\n",
    "valid_type = valid_map[0]\n",
    "test_type = test_map[0]\n",
    "\n",
    "args = {\n",
    "    \"d_model\": 1,\n",
    "    \"d_hidden\": 2,\n",
    "    \"n_layers\": 3,\n",
    "    \"batch_size\": 20,\n",
    "    \"seq_len\": 30,\n",
    "    \"printevery\": 5000,\n",
    "    \"window\": 3,\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 0.0001,\n",
    "    \"dropout\": 0.35,\n",
    "    \"clip\": 2.0,\n",
    "    \"model\": model_type,\n",
    "    \"savename\": model_type.lower(),\n",
    "    \"loadname\": model_type.lower(),\n",
    "    \"trainname\": train_type,\n",
    "    \"validname\": valid_type,\n",
    "    \"testname\": test_type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function\n",
    "def main(args): \n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    params = Params(**args)\n",
    "    train_name = params.trainname\n",
    "    test_name = params.testname\n",
    "    model_type = params.model\n",
    "    d_mod = params.d_model\n",
    "    d_hid = params.d_hidden\n",
    "    dropout = params.dropout\n",
    "    epochs = params.epochs\n",
    "\n",
    "    # real, fake = params.trainname\n",
    "    # [vocab_real, words_real, train_real] = read_encode(real, [], {}, [], 3)\n",
    "    # [vocab_fake, words_fake, train_fake] = read_encode(fake, [], {}, [], 3)\n",
    "\n",
    "    # train_features = torch.cat((torch.tensor(train_real), torch.tensor(train_fake)))\n",
    "    # train_labels = torch.cat((torch.ones(len(train_real)), torch.zeros(len(train_fake))))\n",
    "    # print(f'train_features: {train_features}')\n",
    "    # print(f'train_labels: {train_labels}')\n",
    "\n",
    "    [vocab,words,train] = read_encode(train_name,[],{},[],3)\n",
    "    train_data = torch.tensor(train)\n",
    "    \n",
    "    # print('vocab: %d train: %d' % (len(vocab),len(train)))\n",
    "    # print(f'vocab: {vocab[10:20]}\\n \\n train: {train[10:20]}')\n",
    "    # print(f'fake id: {words[\"[FAKE]\"]}')\n",
    "    # print(f'real id: {words[\"[REAL]\"]}')\n",
    "\n",
    "    [vocab,words,test] = read_encode(test_name,vocab,words,[],-1)\n",
    "    test_data = torch.tensor(test)\n",
    "\n",
    "    #print('vocab: %d test: %d' % (len(vocab),len(test)))\n",
    "    vocab_size = len(vocab)\n",
    "    #train_features, train_targets = process_data(train_data, 'MixTrain')\n",
    "    #test_features, test_targets = process_data(test_data)\n",
    "    \n",
    "    if model_type == 'FFNN':\n",
    "        ffnn_model = FFNN(vocab, words, d_mod, d_hid, dropout)\n",
    "        optimizer = torch.optim.SGD(ffnn_model.parameters(), lr=0.01, momentum=0.9)\n",
    "        #train_loop(ffnn_model, train_features, train_targets, epochs, optimizer)\n",
    "        pass\n",
    "        # print(ffnn_model)\n",
    "#          {add code to instantiate the model, train for K epochs and save model to disk}\n",
    "        \n",
    "    if model_type == 'LSTM':\n",
    "        pass\n",
    "#          {add code to instantiate the model, train for K epochs and save model to disk}\n",
    "\n",
    "    if model_type == 'FFNN_CLASSIFY':\n",
    "        pass\n",
    "#          {add code to instantiate the model, recall model parameters and perform/learn classification}    \n",
    "\n",
    "    if model_type == 'LSTM_CLASSIFY':\n",
    "        pass\n",
    "#          {add code to instantiate the model, recall model parameters and perform/learn classification}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zarif_vfgx7yn\\AppData\\Local\\Temp\\ipykernel_7036\\4022367835.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bio_tensor_list.append(torch.tensor(bio).squeeze())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First bio torch.Size([880])\n",
      "Second bio torch.Size([880])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.9.2 64-bit' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

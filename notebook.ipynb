{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy Contents to starter.py for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zarif_vfgx7yn\\Desktop\\School\\College Classes\\2022-2023\\DL_NLP\\hw#1\\notebook.ipynb Cell 2\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zarif_vfgx7yn/Desktop/School/College%20Classes/2022-2023/DL_NLP/hw%231/notebook.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Imports\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zarif_vfgx7yn/Desktop/School/College%20Classes/2022-2023/DL_NLP/hw%231/notebook.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zarif_vfgx7yn/Desktop/School/College%20Classes/2022-2023/DL_NLP/hw%231/notebook.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zarif_vfgx7yn/Desktop/School/College%20Classes/2022-2023/DL_NLP/hw%231/notebook.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\__init__.py:231\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(textwrap\u001b[39m.\u001b[39mdedent(\u001b[39m'''\u001b[39m\n\u001b[0;32m    217\u001b[0m \u001b[39m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[0;32m    218\u001b[0m \u001b[39m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m                or by running Python from a different directory.\u001b[39m\n\u001b[0;32m    227\u001b[0m \u001b[39m            \u001b[39m\u001b[39m'''\u001b[39m)\u001b[39m.\u001b[39mstrip()) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m  \u001b[39m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m __all__ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [name \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mdir\u001b[39m(_C)\n\u001b[0;32m    232\u001b[0m             \u001b[39mif\u001b[39;00m name[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    233\u001b[0m             \u001b[39mnot\u001b[39;00m name\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39mBase\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m    236\u001b[0m     \u001b[39m# issue 38137 and python issue 43367. Submodules of a C extension are\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[39m# non-standard, and attributes of those submodules cannot be pickled since\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     \u001b[39m# pickle expect to be able to import them as \"from _C.sub import attr\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[39m# which fails with \"_C is not a package\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m \u001b[39mdir\u001b[39m(_C):\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding and decoding\n",
    "def decode(vocab,corpus):\n",
    "    \n",
    "    text = ''\n",
    "    for i in range(len(corpus)):\n",
    "        wID = corpus[i]\n",
    "        text = text + vocab[wID] + ' '\n",
    "    return(text)\n",
    "\n",
    "def encode(words,text):\n",
    "    corpus = []\n",
    "    tokens = text.split(' ')\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            wID = words[t][0]\n",
    "        except:\n",
    "            wID = words['<unk>'][0]\n",
    "        corpus.append(wID)\n",
    "    return(corpus)\n",
    "\n",
    "def read_encode(file_name,vocab,words,corpus,threshold):\n",
    "    \n",
    "    wID = len(vocab)\n",
    "    \n",
    "    if threshold > -1:\n",
    "        with open(file_name,'rt', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                line = line.replace('\\n','')\n",
    "                tokens = line.split(' ')\n",
    "                for t in tokens:\n",
    "                    try:\n",
    "                        elem = words[t]\n",
    "                    except:\n",
    "                        elem = [wID,0]\n",
    "                        vocab.append(t)\n",
    "                        wID = wID + 1\n",
    "                    elem[1] = elem[1] + 1\n",
    "                    words[t] = elem\n",
    "\n",
    "        temp = words\n",
    "        words = {}\n",
    "        vocab = []\n",
    "        wID = 0\n",
    "        words['<unk>'] = [wID,100]\n",
    "        vocab.append('<unk>')\n",
    "        for t in temp:\n",
    "            if temp[t][1] >= threshold:\n",
    "                vocab.append(t)\n",
    "                wID = wID + 1\n",
    "                words[t] = [wID,temp[t][1]]\n",
    "            \n",
    "                    \n",
    "    with open(file_name,'rt', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.replace('\\n','')\n",
    "            tokens = line.split(' ')\n",
    "            for t in tokens:\n",
    "                try:\n",
    "                    wID = words[t][0]\n",
    "                except:\n",
    "                    wID = words['<unk>'][0]\n",
    "                corpus.append(wID)\n",
    "                \n",
    "    return [vocab,words,corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward Model\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, vocab, words, d_model, d_hidden, dropout):\n",
    "        super().__init__() \n",
    "    \n",
    "        # Class parameters\n",
    "        self.vocab = vocab\n",
    "        self.words = words\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.d_model = d_model\n",
    "        self.d_hidden = d_hidden\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embeds = nn.Embedding(self.vocab_size,self.d_model)\n",
    "\n",
    "        # Linear Layer\n",
    "        self.fc1 = nn.Linear(d_model, d_hidden)\n",
    "\n",
    "        # Nonlinear Layer\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, src):\n",
    "        embeds = self.dropout(self.embeds(src))\n",
    "        x = self.activation(self.fc1(embeds))\n",
    "        return x\n",
    "                \n",
    "    def init_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,vocab,words,d_model,d_hidden,n_layers,dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Class Parameters\n",
    "        self.vocab = vocab\n",
    "        self.words = words\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.n_layers = n_layers\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embeds = nn.Embedding(self.vocab_size,self.d_model)\n",
    "        \n",
    "    # Forward\n",
    "    def forward(self,src,h):\n",
    "        embeds = self.dropout(self.embeds(src))       \n",
    "        return [preds,h]\n",
    "    \n",
    "    def init_weights(self):\n",
    "        pass        \n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        return [hidden, cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {0: 'FFNN', 1: 'LSTM', 2: 'FFNN_CLASSIFY', 3: 'LSTM_CLASSIFY'}\n",
    "train_map = {0: 'data/real.train.tok', 1: 'data/fake.train.tok', 2: 'data/mix.train.tok'}\n",
    "valid_map = {0: 'data/real.valid.tok', 1: 'data/fake.valid.tok', 2: 'data/mix.valid.tok'}\n",
    "test_map = {0: 'data/real.test.tok', 1: 'data/fake.test.tok', 3: 'data/mix.test.tok', 4: 'data/blind.test.tok'}\n",
    "\n",
    "model_type = model_map[0]\n",
    "train_type = train_map[0]\n",
    "valid_type = valid_map[0]\n",
    "test_type = test_map[0]\n",
    "\n",
    "args = {\n",
    "    \"d_model\": 1,\n",
    "    \"d_hidden\": 2,\n",
    "    \"n_layers\": 3,\n",
    "    \"batch_size\": 20,\n",
    "    \"seq_len\": 30,\n",
    "    \"printevery\": 5000,\n",
    "    \"window\": 3,\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 0.0001,\n",
    "    \"dropout\": 0.35,\n",
    "    \"clip\": 2.0,\n",
    "    \"model\": model_type,\n",
    "    \"savename\": model_type.lower(),\n",
    "    \"loadname\": model_type.lower(),\n",
    "    \"trainname\": train_type,\n",
    "    \"validname\": valid_type,\n",
    "    \"testname\": test_type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function\n",
    "def main(args): \n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    params = Params(**args)\n",
    "    train_name = params.trainname\n",
    "    test_name = params.testname\n",
    "    model_type = params.model\n",
    "\n",
    "\n",
    "    [vocab,words,train] = read_encode(train_name,[],{},[],3)\n",
    "    print('vocab: %d train: %d' % (len(vocab),len(train)))\n",
    "    [vocab,words,test] = read_encode(test_name,vocab,words,[],-1)\n",
    "    print('vocab: %d test: %d' % (len(vocab),len(test)))\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    if model_type == 'FFNN':\n",
    "        pass\n",
    "#          {add code to instantiate the model, train for K epochs and save model to disk}\n",
    "        \n",
    "    if model_type == 'LSTM':\n",
    "        pass\n",
    "#          {add code to instantiate the model, train for K epochs and save model to disk}\n",
    "\n",
    "    if model_type == 'FFNN_CLASSIFY':\n",
    "        pass\n",
    "#          {add code to instantiate the model, recall model parameters and perform/learn classification}\n",
    "\n",
    "    if model_type == 'LSTM_CLASSIFY':\n",
    "        pass\n",
    "#          {add code to instantiate the model, recall model parameters and perform/learn classification}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zarif_vfgx7yn\\Desktop\\School\\College Classes\\2022-2023\\DL_NLP\\hw#1\\notebook.ipynb Cell 11\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zarif_vfgx7yn/Desktop/School/College%20Classes/2022-2023/DL_NLP/hw%231/notebook.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main(args)\n",
      "\u001b[1;32mc:\\Users\\zarif_vfgx7yn\\Desktop\\School\\College Classes\\2022-2023\\DL_NLP\\hw#1\\notebook.ipynb Cell 11\u001b[0m in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zarif_vfgx7yn/Desktop/School/College%20Classes/2022-2023/DL_NLP/hw%231/notebook.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m(args): \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zarif_vfgx7yn/Desktop/School/College%20Classes/2022-2023/DL_NLP/hw%231/notebook.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zarif_vfgx7yn/Desktop/School/College%20Classes/2022-2023/DL_NLP/hw%231/notebook.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     params \u001b[39m=\u001b[39m Params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zarif_vfgx7yn/Desktop/School/College%20Classes/2022-2023/DL_NLP/hw%231/notebook.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_name \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mtrainname\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
